{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "ppg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksg4pPcCWcJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66539805-c172-4bc7-b0b6-2c50c9475bf9"
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib\n",
        "import pickle\n",
        "\n",
        "\n",
        "ssh_config = \"\"\"\n",
        "Host github.com\n",
        "  IdentityFile ~/.ssh/github.pem\n",
        "  User davipeag\n",
        "  StrictHostKeyChecking no\n",
        "\"\"\"\n",
        "\n",
        "if os.name == 'nt':\n",
        "  base_path = \"\"\n",
        "  REPO_DIR = \".\"\n",
        "  STORE_DIR =\".\" \n",
        "  print(\"Windows\")\n",
        "else:\n",
        "  print(\"Unix-like\")\n",
        "  REPO_DIR = \"/tmp/HeartRateRegression\"\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  GIT_PATH = \"/content/drive/My\\ Drive/deeplearning_project/github.pem\"\n",
        "  DATA_DIR = os.path.join(REPO_DIR, \"repo\")\n",
        "  STORE_DIR =\"/content/drive/My Drive/deeplearning_project/\" \n",
        "  !mkdir ~/.ssh\n",
        "  !cp -u {GIT_PATH} ~/.ssh/\n",
        "  !chmod u=rw,g=,o= ~/.ssh/github.pem\n",
        "  !echo \"{ssh_config}\" > ~/.ssh/config\n",
        "  !chmod u=rw,g=,o= ~/.ssh/config\n",
        "  ! (cd /tmp && git clone git@github.com:davipeag/HeartRateRegression.git)\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "  import sys\n",
        "  sys.path.append(REPO_DIR)\n",
        "\n",
        "def git_pull():\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "\n",
        "git_pull()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Unix-like\n",
            "Mounted at /content/drive\n",
            "Cloning into 'HeartRateRegression'...\n",
            "Warning: Permanently added 'github.com,192.30.255.113' (RSA) to the list of known hosts.\n",
            "remote: Enumerating objects: 314, done.\u001b[K\n",
            "remote: Counting objects: 100% (314/314), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 2036 (delta 169), reused 189 (delta 70), pack-reused 1722\u001b[K\n",
            "Receiving objects: 100% (2036/2036), 131.64 MiB | 26.14 MiB/s, done.\n",
            "Resolving deltas: 100% (1314/1314), done.\n",
            "Warning: Permanently added the RSA host key for IP address '192.30.255.112' to the list of known hosts.\n",
            "Already up to date.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCFiZv0xM1pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36d943a-e028-4147-e8e8-945fd9caaa20"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "args = {\n",
        "    'epoch_num': 250,     # Number of epochs.\n",
        "    'lr': 1.0e-3,           # Learning rate.\n",
        "    'weight_decay': 10e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 0,     # Number of workers on data loader.\n",
        "    'batch_size': 128,     # Mini-batch size. 128\n",
        "    'batch_test': 248,     # size of test batch\n",
        "    'window': 15,\n",
        "    'initial_window':5,\n",
        "    'clip_norm': 6.0,     # Upper limit on gradient L2 norm ###\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "SEED = 1234\n",
        "def reset_seeds():\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reset_seeds()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7V97F8pWmvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1be802e-34cb-41e9-bceb-8d431865c95f"
      },
      "source": [
        "from data_utils import (PpgDaliaExtractor, FormatPPGDalia)\n",
        "\n",
        "extractor = PpgDaliaExtractor(DATA_DIR)\n",
        "ppg_dalia_formatter = FormatPPGDalia()\n",
        "dfs_train = [ppg_dalia_formatter.transform(extractor.extract_subject(i)) for i in range(1,16)]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdDUK1vToJWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237c0faf-9fc0-4ac0-cb03-cfd285b2a411"
      },
      "source": [
        "git_pull()\n",
        "\n",
        "import importlib\n",
        "\n",
        "import PPG\n",
        "\n",
        "from PPG import FullTrainer\n",
        "\n",
        "importlib.reload(PPG.AttentionDefaults)\n",
        "importlib.reload(PPG)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(PPG.Models)\n",
        "importlib.reload(PPG.NoHrPceLstmModel)\n",
        "importlib.reload(PPG.TrainerXY)\n",
        "importlib.reload(PPG.TrainerIS)\n",
        "importlib.reload(PPG.FullTrainer)\n",
        "\n",
        "git_pull()\n",
        "\n",
        "import importlib\n",
        "\n",
        "import PPG\n",
        "import preprocessing_utils\n",
        "from PPG import FullTrainer\n",
        "import RegressionHR\n",
        "\n",
        "from RegressionHR import FullTrainer\n",
        "from RegressionHR import PceLstmDefaults\n",
        "from RegressionHR import PceLstmModel\n",
        "from RegressionHR import TrainerJoint\n",
        "from RegressionHR import  UtilitiesData\n",
        "import Models\n",
        "from  Models.BaseModels import ConstantHiddenSizeHalvingFullyConvolutionalEncoder1D\n",
        "from Models import TimeSnippetModels\n",
        "\n",
        "import PreprocessingHelpers\n",
        "\n",
        "import Constants\n",
        "from Constants import DatasetMapping\n",
        "\n",
        "\n",
        "import Trainer\n",
        "from  Trainer import BatchTrainers\n",
        "from Trainer import BatchComputers\n",
        "from Trainer import Interfaces\n",
        "\n",
        "import CustomTrainers\n",
        "from CustomTrainers import  DeepConvLstmFullTrainer\n",
        "from CustomTrainers import SpecificTrainers, FullTrainers\n",
        "\n",
        "import PPG\n",
        "\n",
        "from PreprocessingHelpers import TransformerGetters\n",
        "\n",
        "importlib.reload(PreprocessingHelpers)\n",
        "importlib.reload(PreprocessingHelpers.TransformerGetters)\n",
        "importlib.reload(PreprocessingHelpers)\n",
        "\n",
        "importlib.reload(PPG)\n",
        "importlib.reload(PPG.NoHrPceLstmModel)\n",
        "importlib.reload(PPG.TrainerXY)\n",
        "importlib.reload(PPG.TrainerIS)\n",
        "importlib.reload(PPG.FullTrainer)\n",
        "importlib.reload(PceLstmDefaults)\n",
        "\n",
        "\n",
        "\n",
        "importlib.reload(CustomTrainers)\n",
        "importlib.reload(CustomTrainers.FullTrainers)\n",
        "importlib.reload(CustomTrainers.SpecificTrainers)\n",
        "\n",
        "import PreprocessingHelpers.TransformerGetters\n",
        "                            \n",
        "\n",
        "\n",
        "importlib.reload(Constants.DatasetMapping)\n",
        "\n",
        "\n",
        "# importlib.reload(PPG.AttentionDefaults)\n",
        "\n",
        "# importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(PPG.Models)\n",
        "importlib.reload(Models.BaseModels)\n",
        "importlib.reload(Models.TimeSnippetModels)\n",
        "importlib.reload(preprocessing_utils)\n",
        "importlib.reload(RegressionHR)\n",
        "importlib.reload(RegressionHR.FullTrainer)\n",
        "importlib.reload(RegressionHR.PceLstmDefaults)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(preprocessing_utils)\n",
        "importlib.reload(RegressionHR.TrainerJoint)\n",
        "importlib.reload(RegressionHR.UtilitiesData)\n",
        "importlib.reload(RegressionHR.PceLstmModel)\n",
        "importlib.reload(preprocessing_utils)\n",
        "\n",
        "importlib.reload(Trainer)\n",
        "importlib.reload(Trainer.BatchTrainers)\n",
        "importlib.reload(Trainer.BatchComputers)\n",
        "importlib.reload(Trainer.ToolBox)\n",
        "importlib.reload(Trainer.Interfaces )\n",
        "# import imp\n",
        "# for module in sys.modules.values():\n",
        "#     importlib.reload(module)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects:  25% (1/4)   \rUnpacking objects:  50% (2/4)   \rUnpacking objects:  75% (3/4)   \rUnpacking objects: 100% (4/4)   \rUnpacking objects: 100% (4/4), done.\n",
            "From github.com:davipeag/HeartRateRegression\n",
            "   2b5fd6a..1485ead  master     -> origin/master\n",
            "Updating 2b5fd6a..1485ead\n",
            "Fast-forward\n",
            " PPG/NoHrPceLstmModel.py | 3 \u001b[32m+++\u001b[m\n",
            " 1 file changed, 3 insertions(+)\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'Trainer.Interfaces' from '/tmp/HeartRateRegression/Trainer/Interfaces.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGYuadHNu5K8"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBzugix7u5K8",
        "outputId": "ae2e7322-1c98-4d5f-ba30-4f6845c5515d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def compute_ensemble_named(results, model_name=\"PceLstm\"):\n",
        "  \n",
        "  # results = [r[model_name] for r in results]\n",
        "\n",
        "  ys = [v[\"labels\"].reshape(-1) for v in results]\n",
        "  min_len_y = min([len(y) for y in ys])\n",
        "  ys = [y[:min_len_y] for y in ys]\n",
        "  for i in range(1, len(ys)-1):\n",
        "    # assert np.all(ys[i] == ys[i-1])\n",
        "    assert np.all(np.abs(ys[i] - ys[i-1])<1)\n",
        "  ps = np.stack([v[\"predictions\"].reshape(-1)[:min_len_y] for v in results])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "  plt.show()\n",
        "\n",
        "  return np.mean(np.abs(a - y)), np.mean(np.abs(ps - y))\n",
        "\n",
        "fchoice = {\n",
        "    'is_h_size': 64,\n",
        "    'ts_per_is': 12,\n",
        "    'period_s': 8,\n",
        "    'step_s': 2,\n",
        "    'ts_per_window': 50,\n",
        "    'ts_sub': 0,\n",
        "    'batch_size': 64,\n",
        "    'weight_decay': 1e-05,\n",
        "    'lr': 0.005,\n",
        "    'nattrs': 7,\n",
        "    'dropout_rate':0.15,\n",
        "    'lstm_input': 128,\n",
        "    'lstm_size': 64,\n",
        "    'ts_h_size': 16\n",
        "}\n",
        "import CustomTrainers\n",
        "from CustomTrainers import SingleNoHrPpgPceLstmFullTrainer\n",
        "\n",
        "from PPG import UtilitiesDataXY\n",
        "from collections import defaultdict\n",
        "\n",
        "model_name = \"NoHrPpgPceLstm\"\n",
        "nepoch = 50\n",
        "aresults = defaultdict(dict)\n",
        "for val_sub in range(7):\n",
        "  for ts_sub in range(15):\n",
        "    fchoice['ts_sub'] = ts_sub\n",
        "    full_trainer = SingleNoHrPpgPceLstmFullTrainer(dfs_train, args[\"device\"], nepoch, \"dalia\",\"NoHrPpgPceLstm\")\n",
        "    filename = f\"dalia_ts_{ts_sub}_{val_sub}_from_ecg_period2_new_parameters_nepoch_{nepoch}_.pkl\"\n",
        "    save_path = os.path.join(STORE_DIR, filename)\n",
        "    try:\n",
        "      with open(save_path , \"rb\") as f:\n",
        "        out = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "      out = full_trainer.train(**fchoice)\n",
        "      with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(out, f)    \n",
        "    print(out[\"args\"], out['metric'])\n",
        "    aresults[ts_sub][val_sub] = out\n",
        "    print(f\"{ts_sub}-TS:{compute_ensemble_named(list(aresults[ts_sub].values()))}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best val epoch: 1\n",
            "[1/50]: loss_train: [15.02295] loss_val [13.851043] loss_ts [15.52278]\n",
            "best val epoch: 2\n",
            "[2/50]: loss_train: [12.511559] loss_val [11.493678] loss_ts [12.527854]\n",
            "best val epoch: 3\n",
            "[3/50]: loss_train: [12.3784895] loss_val [11.305831] loss_ts [8.514768]\n",
            "best val epoch: 4\n",
            "[4/50]: loss_train: [9.087586] loss_val [8.37295] loss_ts [9.789338]\n",
            "best val epoch: 5\n",
            "[5/50]: loss_train: [7.5194035] loss_val [7.4277477] loss_ts [8.746746]\n",
            "best val epoch: 6\n",
            "[6/50]: loss_train: [7.348388] loss_val [6.8422236] loss_ts [6.447276]\n",
            "best val epoch: 7\n",
            "[7/50]: loss_train: [6.3584824] loss_val [6.2353363] loss_ts [6.5673237]\n",
            "best val epoch: 10\n",
            "[10/50]: loss_train: [5.7114124] loss_val [5.8158402] loss_ts [7.145897]\n",
            "best val epoch: 11\n",
            "[11/50]: loss_train: [5.386939] loss_val [5.490951] loss_ts [6.5174084]\n",
            "best val epoch: 14\n",
            "[14/50]: loss_train: [4.847815] loss_val [5.2371125] loss_ts [7.535503]\n",
            "best val epoch: 15\n",
            "[15/50]: loss_train: [4.833465] loss_val [5.127008] loss_ts [5.7974644]\n",
            "best val epoch: 17\n",
            "[17/50]: loss_train: [4.741568] loss_val [4.9996037] loss_ts [5.634897]\n",
            "best val epoch: 20\n",
            "[20/50]: loss_train: [4.2456493] loss_val [4.878577] loss_ts [5.4830694]\n",
            "best val epoch: 26\n",
            "[26/50]: loss_train: [4.019447] loss_val [4.637464] loss_ts [7.0726285]\n",
            "best val epoch: 27\n",
            "[27/50]: loss_train: [3.9428358] loss_val [4.554863] loss_ts [5.8090096]\n",
            "best val epoch: 29\n",
            "[29/50]: loss_train: [3.59744] loss_val [4.285811] loss_ts [5.4012794]\n",
            "best val epoch: 35\n",
            "[35/50]: loss_train: [3.4888082] loss_val [4.15531] loss_ts [5.9238625]\n",
            "best val epoch: 45\n",
            "[45/50]: loss_train: [3.2016258] loss_val [4.1415744] loss_ts [6.25635]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL7zop0WGrHb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "# fchoice = {'val_sub': 4,\n",
        "#   'ts_sub': 0,\n",
        "#   'batch_size': 64,\n",
        "#   'weight_decay': 0,\n",
        "#   'lr': 0.001,\n",
        "#   'nattrs': 5,\n",
        "#   'bvp_count': 16,\n",
        "#   'dropout_rate': 0.25,\n",
        "#   'lstm_input': 128,\n",
        "#   'lstm_size': 64,\n",
        "#   'ts_h_size': 64\n",
        "#   }\n",
        "def compute_ensemble(results):\n",
        "  ps = np.stack([v[\"predictions\"][1].reshape(-1).numpy() for v in results])\n",
        "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
        "\n",
        "  for i in range(1, len(ys)-1):\n",
        "    assert np.all(ys[i] == ys[i-1])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "\n",
        "  return np.mean(np.abs(a - y)), np.mean(np.abs(ps - y))\n",
        "\n",
        "\n",
        "fchoice = {'val_sub': 4,\n",
        "  'ts_sub': 0,\n",
        "  'batch_size': 64,\n",
        "  'weight_decay': 0.0001,\n",
        "  'lr': 0.001,\n",
        "  'nattrs': 5,\n",
        "  'bvp_count': 12,\n",
        "  'dropout_rate': 0,\n",
        "  'lstm_input': 128,\n",
        "  'lstm_size': 32,\n",
        "  'ts_h_size': 32\n",
        "  }\n",
        "\n",
        "\n",
        "from PPG import UtilitiesDataXY\n",
        "from collections import defaultdict\n",
        "\n",
        "aresults = defaultdict(list)\n",
        "for i in range(7):\n",
        "  for ts_sub in [0,1,2,3, 4,5,6,7,8,9,10,11,12,13,14]:\n",
        "    filename = f\"dalia_ts_{ts_sub}_{i}_from_ecg_period2_.pkl\"\n",
        "    save_path = os.path.join(STORE_DIR, filename)\n",
        "    try:\n",
        "      with open(save_path , \"rb\") as f:\n",
        "        out = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "      full_trainer = FullTrainer.JointValNoHrPceLstmFullTrainer(dfs_train, args[\"device\"])\n",
        "      try:\n",
        "        fchoice[\"ts_sub\"] = ts_sub\n",
        "        out = full_trainer.train(**fchoice)\n",
        "        \n",
        "        with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(out, f)\n",
        "\n",
        "      except RuntimeError as e:\n",
        "        if isinstance(e, KeyboardInterrupt):\n",
        "          raise e\n",
        "        else:\n",
        "          print(\"####\")\n",
        "          print(f\"Failed: {choice}\")\n",
        "          print(\"###\")\n",
        "    \n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    aresults[ts_sub].append(out)\n",
        "    print(f\"{ts_sub}-TS:{compute_ensemble(aresults[ts_sub])}\")\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC7RYiJL3nh-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjYv8BlUKYnM"
      },
      "source": [
        "\n",
        "def compute_ensemble(results):\n",
        "  ps = [v[\"predictions\"][1].reshape(-1).numpy() for v in results]\n",
        "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
        "\n",
        "  for i in range(1, len(ys)-1):\n",
        "    assert np.all(ys[i] == ys[i-1])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "\n",
        "  np.mean(np.abs(a - y))\n",
        "\n",
        "compute_ensemble(dresults)\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# p = p.reshape(-1)\n",
        "\n",
        "# plt.plot(y)\n",
        "# plt.plot(p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdPECC3jYpjp"
      },
      "source": [
        "import random \n",
        "options = {\n",
        "  \"ts_h_size\": [64, 128],\n",
        "  \"lstm_size\": [64, 64, 128],\n",
        "  \"lstm_input\": [64, 128, 256],\n",
        "  \"dropout_rate\": [0.25],\n",
        "  \"bvp_count\": [8,16],\n",
        "  \"nattrs\": [5],\n",
        "  'lr': [0.001],\n",
        "  'weight_decay': [0, 0.0001],\n",
        "  'batch_size': [64, 128, 256],\n",
        "  'ts_sub': [0],\n",
        "  'val_sub': [4]\n",
        " }\n",
        "\n",
        "def choose(options):\n",
        "  choice = dict()\n",
        "  for k,v in options.items():\n",
        "    choice[k] = random.choice(v)\n",
        "  return choice\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYFUSi1-4nzj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YFiismva8BL"
      },
      "source": [
        "results = list()\n",
        "from PPG import UtilitiesDataXY\n",
        "while True:\n",
        "  full_trainer = FullTrainer.NoHrPceLstmFullTrainer(dfs_train, args[\"device\"])\n",
        "  choice = choose(options)\n",
        "  try:\n",
        "    out = full_trainer.train(**choice)\n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    results.append([out[\"args\"], out[\"metric\"]])\n",
        "  except RuntimeError as e:\n",
        "    if isinstance(e, KeyboardInterrupt):\n",
        "      raise e\n",
        "    else:\n",
        "      print(\"####\")\n",
        "      print(f\"Failed: {choice}\")\n",
        "      print(\"###\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNTrC3_M2D-Z"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chhDawFV_14T"
      },
      "source": [
        "fchoice = {'val_sub': 4,\n",
        "  'ts_sub': 0,\n",
        "  'batch_size': 64,\n",
        "  'weight_decay': 0,\n",
        "  'lr': 0.001,\n",
        "  'nattrs': 5,\n",
        "  'bvp_count': 16,\n",
        "  'dropout_rate': 0.25,\n",
        "  'lstm_input': 128,\n",
        "  'lstm_size': 64,\n",
        "  'ts_h_size': 64}\n",
        "\n",
        "\n",
        "dresults = list()\n",
        "from PPG import UtilitiesDataXY\n",
        "ts_sub = 3\n",
        "for val_sub in [i for i in range(15) if i != ts_sub]:\n",
        "  full_trainer = FullTrainer.NoHrPceLstmFullTrainer(dfs_train, args[\"device\"])\n",
        "  try:\n",
        "    fchoice[\"ts_sub\"] = ts_sub\n",
        "    fchoice[\"val_sub\"] = val_sub\n",
        "    out = full_trainer.train(**fchoice)\n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    dresults.append([out])\n",
        "  except RuntimeError as e:\n",
        "    if isinstance(e, KeyboardInterrupt):\n",
        "      raise e\n",
        "    else:\n",
        "      print(\"####\")\n",
        "      print(f\"Failed: {choice}\")\n",
        "      print(\"###\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjMx66OD9T9"
      },
      "source": [
        "y = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFOG4ar9WYpt"
      },
      "source": [
        "full_trainer = FullTrainer.AttentionFullTrainer(dfs_train, args[\"device\"], 0, 1)\n",
        "\n",
        "full_trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYVLTnMGXYGp"
      },
      "source": [
        "from PPG import UtilitiesDataXY \n",
        "\n",
        "\n",
        "transformers = PPG.AttentionDefaults.get_preprocessing_transformer()\n",
        "make_loaders = UtilitiesDataXY.DataLoaderFactory(transformers, dfs_train).make_loaders\n",
        "\n",
        "loader_tr, loader_val, loader_ts = make_loaders(ts_sub=0, val_sub=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Vr-YDyoGfH"
      },
      "source": [
        "from PPG.Models import SnippetConvolutionalTransformer\n",
        "\n",
        "net = SnippetConvolutionalTransformer().to(args[\"device\"])\n",
        "\n",
        "# x,y = next(iter(loader_tr))\n",
        "\n",
        "# p = net(x)\n",
        "\n",
        "criterion = nn.MSELoss().to(args[\"device\"])# nn.L1Loss().to(args[\"device\"]) #nn.CrossEntropyLoss().to(args[\"device\"])\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=args[\"lr\"],\n",
        "                             weight_decay=args[\"weight_decay\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ktvTpTaOcy"
      },
      "source": [
        "from PPG.TrainerXY import (EpochTrainerXY, MetricsComputerXY, TrainHelperXY)\n",
        "from preprocessing_utils import ZTransformer2\n",
        "\n",
        "epoch_trainer = EpochTrainerXY(net, optimizer, criterion, args[\"device\"])\n",
        "ztransformer = ZTransformer2(['heart_rate', 'wrist-ACC-0', 'wrist-ACC-1', 'wrist-ACC-2',\n",
        "              'wrist-BVP-0', 'wrist-EDA-0', 'wrist-TEMP-0', 'chest-ACC-0',\n",
        "              'chest-ACC-1', 'chest-ACC-2', 'chest-Resp-0'])\n",
        "metrics_comuter = MetricsComputerXY(ztransformer)\n",
        "\n",
        "train_helper = TrainHelperXY(epoch_trainer, loader_tr, loader_val, loader_ts, metrics_comuter.mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lyVMlfzkaFA"
      },
      "source": [
        "train_helper.train(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0tSXLWPuCQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}