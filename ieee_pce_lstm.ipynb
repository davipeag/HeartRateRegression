{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "ppg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksg4pPcCWcJR",
        "outputId": "0f214d96-22bf-4738-aa0f-f8ea49237059",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "ssh_config = \"\"\"\n",
        "Host github.com\n",
        "  IdentityFile ~/.ssh/github.pem\n",
        "  User davipeag\n",
        "  StrictHostKeyChecking no\n",
        "\"\"\"\n",
        "\n",
        "if os.name == 'nt':\n",
        "  base_path = \"\"\n",
        "  REPO_DIR = \".\"\n",
        "  STORE_DIR =\".\" \n",
        "  print(\"Windows\")\n",
        "else:\n",
        "  print(\"Unix-like\")\n",
        "  REPO_DIR = \"/tmp/HeartRateRegression\"\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  GIT_PATH = \"/content/drive/My\\ Drive/deeplearning_project/github.pem\"\n",
        "  DATA_DIR = os.path.join(REPO_DIR, \"repo\")\n",
        "  STORE_DIR =\"/content/drive/My Drive/deeplearning_project/\" \n",
        "  !mkdir ~/.ssh\n",
        "  !cp -u {GIT_PATH} ~/.ssh/\n",
        "  !chmod u=rw,g=,o= ~/.ssh/github.pem\n",
        "  !echo \"{ssh_config}\" > ~/.ssh/config\n",
        "  !chmod u=rw,g=,o= ~/.ssh/config\n",
        "  ! (cd /tmp && git clone git@github.com:davipeag/HeartRateRegression.git)\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "  import sys\n",
        "  sys.path.append(REPO_DIR)\n",
        "\n",
        "def git_pull():\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "\n",
        "git_pull()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=707bce13259abc776090a363b9315039e5aa8e6242be9053b8e2c7e4dbd0762d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Unix-like\n",
            "Mounted at /content/drive\n",
            "Cloning into 'HeartRateRegression'...\n",
            "Warning: Permanently added 'github.com,140.82.114.3' (RSA) to the list of known hosts.\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 756 (delta 17), reused 15 (delta 5), pack-reused 719\u001b[K\n",
            "Receiving objects: 100% (756/756), 87.97 MiB | 33.59 MiB/s, done.\n",
            "Resolving deltas: 100% (473/473), done.\n",
            "Already up to date.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCFiZv0xM1pa",
        "outputId": "22baac8b-2759-4625-8afa-37890f3396bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "args = {\n",
        "    'epoch_num': 250,     # Number of epochs.\n",
        "    'lr': 1.0e-3,           # Learning rate.\n",
        "    'weight_decay': 10e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 0,     # Number of workers on data loader.\n",
        "    'batch_size': 128,     # Mini-batch size. 128\n",
        "    'batch_test': 248,     # size of test batch\n",
        "    'window': 15,\n",
        "    'initial_window':5,\n",
        "    'clip_norm': 6.0,     # Upper limit on gradient L2 norm ###\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "SEED = 1234\n",
        "def reset_seeds():\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reset_seeds()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7V97F8pWmvK",
        "outputId": "5fd47ac9-7ca8-440f-b837-04408311d35d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from data_utils import (FormatIeee,  IeeeExtractor)\n",
        "\n",
        "SUBJECTS = list(range(1,13))\n",
        "\n",
        "extractor = IeeeExtractor(DATA_DIR)\n",
        "formatter = FormatIeee()\n",
        "dfs_train = [formatter.transform(extractor.extract_subject(i)) for i in SUBJECTS]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdDUK1vToJWo"
      },
      "source": [
        "git_pull()\n",
        "\n",
        "import importlib\n",
        "\n",
        "import PPG\n",
        "\n",
        "from PPG import FullTrainer\n",
        "\n",
        "importlib.reload(PPG.AttentionDefaults)\n",
        "importlib.reload(PPG)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(PPG.Models)\n",
        "importlib.reload(PPG.NoHrPceLstmModel)\n",
        "importlib.reload(PPG.TrainerXY)\n",
        "importlib.reload(PPG.TrainerIS)\n",
        "importlib.reload(PPG.FullTrainer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL7zop0WGrHb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# fchoice = {'val_sub': 4,\n",
        "#   'ts_sub': 0,\n",
        "#   'batch_size': 64,\n",
        "#   'weight_decay': 0,\n",
        "#   'lr': 0.001,\n",
        "#   'nattrs': 5,\n",
        "#   'bvp_count': 16,\n",
        "#   'dropout_rate': 0.25,\n",
        "#   'lstm_input': 128,\n",
        "#   'lstm_size': 64,\n",
        "#   'ts_h_size': 64\n",
        "#   }\n",
        "def compute_ensemble(results):\n",
        "  ps = [v[\"predictions\"][1].reshape(-1).numpy() for v in results]\n",
        "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
        "\n",
        "  for i in range(1, len(ys)-1):\n",
        "    assert np.all(ys[i] == ys[i-1])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "\n",
        "  return np.mean(np.abs(a - y))\n",
        "\n",
        "\n",
        "fchoice = {'val_sub': 4,\n",
        "  'ts_sub': 0,\n",
        "  'batch_size': 64,\n",
        "  'weight_decay': 0.0001,\n",
        "  'lr': 0.001,\n",
        "  'nattrs': 5,\n",
        "  'bvp_count': 12,\n",
        "  'dropout_rate': 0,\n",
        "  'lstm_input': 128,\n",
        "  'lstm_size': 32,\n",
        "  'ts_h_size': 32\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "from PPG import UtilitiesDataXY\n",
        "\n",
        "\n",
        "aresults = list()\n",
        "for ts_sub in [0,1,2,3, 4,5,6,7,8,9,10,11,12,13,14]:\n",
        "  dresults = list()\n",
        "  for i in range(7):\n",
        "    filename = f\"ieee_train_ts_{ts_sub}_{i}.pkl\"\n",
        "    save_path = os.path.join(STORE_DIR, filename)\n",
        "    try:\n",
        "      with open(save_path , \"rb\") as f:\n",
        "        out = pickle.load(f)\n",
        "    else:\n",
        "      dresults.append(out)\n",
        "      continue\n",
        "    except FileNotFoundError:\n",
        "      full_trainer = FullTrainer.IeeeJointValAttentionFullTrainer(dfs_train, args[\"device\"])\n",
        "      try:\n",
        "        fchoice[\"ts_sub\"] = ts_sub\n",
        "        out = full_trainer.train(**fchoice)\n",
        "        print(out[\"args\"], out[\"metric\"])\n",
        "        dresults.append(out)\n",
        "        \n",
        "        \n",
        "        with open(save_path, \"wb\") as f:\n",
        "          results = pickle.dump(out, f)\n",
        "\n",
        "      except RuntimeError as e:\n",
        "        if isinstance(e, KeyboardInterrupt):\n",
        "          raise e\n",
        "        else:\n",
        "          print(\"####\")\n",
        "          print(f\"Failed: {choice}\")\n",
        "          print(\"###\")\n",
        "  print(f\"TS:{compute_ensemble(dresults)}\")\n",
        "  aresults.append(dresults)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjYv8BlUKYnM"
      },
      "source": [
        "\n",
        "def compute_ensemble(results):\n",
        "  ps = [v[\"predictions\"][1].reshape(-1).numpy() for v in results]\n",
        "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
        "\n",
        "  for i in range(1, len(ys)-1):\n",
        "    assert np.all(ys[i] == ys[i-1])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "\n",
        "  np.mean(np.abs(a - y))\n",
        "\n",
        "compute_ensemble(dresults)\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# p = p.reshape(-1)\n",
        "\n",
        "# plt.plot(y)\n",
        "# plt.plot(p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdPECC3jYpjp"
      },
      "source": [
        "import random \n",
        "options = {\n",
        "  \"ts_h_size\": [64, 128],\n",
        "  \"lstm_size\": [64, 64, 128],\n",
        "  \"lstm_input\": [64, 128, 256],\n",
        "  \"dropout_rate\": [0.25],\n",
        "  \"bvp_count\": [8,16],\n",
        "  \"nattrs\": [5],\n",
        "  'lr': [0.001],\n",
        "  'weight_decay': [0, 0.0001],\n",
        "  'batch_size': [64, 128, 256],\n",
        "  'ts_sub': [0],\n",
        "  'val_sub': [4]\n",
        " }\n",
        "\n",
        "def choose(options):\n",
        "  choice = dict()\n",
        "  for k,v in options.items():\n",
        "    choice[k] = random.choice(v)\n",
        "  return choice\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYFUSi1-4nzj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YFiismva8BL"
      },
      "source": [
        "results = list()\n",
        "from PPG import UtilitiesDataXY\n",
        "while True:\n",
        "  full_trainer = FullTrainer.NoHrPceLstmFullTrainer(dfs_train, args[\"device\"])\n",
        "  choice = choose(options)\n",
        "  try:\n",
        "    out = full_trainer.train(**choice)\n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    results.append([out[\"args\"], out[\"metric\"]])\n",
        "  except RuntimeError as e:\n",
        "    if isinstance(e, KeyboardInterrupt):\n",
        "      raise e\n",
        "    else:\n",
        "      print(\"####\")\n",
        "      print(f\"Failed: {choice}\")\n",
        "      print(\"###\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNTrC3_M2D-Z"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chhDawFV_14T"
      },
      "source": [
        "fchoice = {'val_sub': 4,\n",
        "  'ts_sub': 0,\n",
        "  'batch_size': 64,\n",
        "  'weight_decay': 0,\n",
        "  'lr': 0.001,\n",
        "  'nattrs': 5,\n",
        "  'bvp_count': 16,\n",
        "  'dropout_rate': 0.25,\n",
        "  'lstm_input': 128,\n",
        "  'lstm_size': 64,\n",
        "  'ts_h_size': 64}\n",
        "\n",
        "\n",
        "dresults = list()\n",
        "from PPG import UtilitiesDataXY\n",
        "ts_sub = 3\n",
        "for val_sub in [i for i in range(15) if i != ts_sub]:\n",
        "  full_trainer = FullTrainer.NoHrPceLstmFullTrainer(dfs_train, args[\"device\"])\n",
        "  try:\n",
        "    fchoice[\"ts_sub\"] = ts_sub\n",
        "    fchoice[\"val_sub\"] = val_sub\n",
        "    out = full_trainer.train(**fchoice)\n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    dresults.append([out])\n",
        "  except RuntimeError as e:\n",
        "    if isinstance(e, KeyboardInterrupt):\n",
        "      raise e\n",
        "    else:\n",
        "      print(\"####\")\n",
        "      print(f\"Failed: {choice}\")\n",
        "      print(\"###\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjMx66OD9T9"
      },
      "source": [
        "y = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFOG4ar9WYpt"
      },
      "source": [
        "full_trainer = FullTrainer.AttentionFullTrainer(dfs_train, args[\"device\"], 0, 1)\n",
        "\n",
        "full_trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYVLTnMGXYGp"
      },
      "source": [
        "from PPG import UtilitiesDataXY \n",
        "\n",
        "\n",
        "transformers = PPG.AttentionDefaults.get_preprocessing_transformer()\n",
        "make_loaders = UtilitiesDataXY.DataLoaderFactory(transformers, dfs_train).make_loaders\n",
        "\n",
        "loader_tr, loader_val, loader_ts = make_loaders(ts_sub=0, val_sub=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Vr-YDyoGfH"
      },
      "source": [
        "from PPG.Models import SnippetConvolutionalTransformer\n",
        "\n",
        "net = SnippetConvolutionalTransformer().to(args[\"device\"])\n",
        "\n",
        "# x,y = next(iter(loader_tr))\n",
        "\n",
        "# p = net(x)\n",
        "\n",
        "criterion = nn.MSELoss().to(args[\"device\"])# nn.L1Loss().to(args[\"device\"]) #nn.CrossEntropyLoss().to(args[\"device\"])\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=args[\"lr\"],\n",
        "                             weight_decay=args[\"weight_decay\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ktvTpTaOcy"
      },
      "source": [
        "from PPG.TrainerXY import (EpochTrainerXY, MetricsComputerXY, TrainHelperXY)\n",
        "from preprocessing_utils import ZTransformer2\n",
        "\n",
        "epoch_trainer = EpochTrainerXY(net, optimizer, criterion, args[\"device\"])\n",
        "ztransformer = ZTransformer2(['heart_rate', 'wrist-ACC-0', 'wrist-ACC-1', 'wrist-ACC-2',\n",
        "              'wrist-BVP-0', 'wrist-EDA-0', 'wrist-TEMP-0', 'chest-ACC-0',\n",
        "              'chest-ACC-1', 'chest-ACC-2', 'chest-Resp-0'])\n",
        "metrics_comuter = MetricsComputerXY(ztransformer)\n",
        "\n",
        "train_helper = TrainHelperXY(epoch_trainer, loader_tr, loader_val, loader_ts, metrics_comuter.mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lyVMlfzkaFA"
      },
      "source": [
        "train_helper.train(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0tSXLWPuCQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}