{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "ppg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksg4pPcCWcJR",
        "outputId": "83f591d0-d55e-4355-83ad-ee124046f438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "ssh_config = \"\"\"\n",
        "Host github.com\n",
        "  IdentityFile ~/.ssh/github.pem\n",
        "  User davipeag\n",
        "  StrictHostKeyChecking no\n",
        "\"\"\"\n",
        "\n",
        "if os.name == 'nt':\n",
        "  base_path = \"\"\n",
        "  REPO_DIR = \".\"\n",
        "  STORE_DIR =\".\" \n",
        "  print(\"Windows\")\n",
        "else:\n",
        "  print(\"Unix-like\")\n",
        "  REPO_DIR = \"/tmp/HeartRateRegression\"\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  GIT_PATH = \"/content/drive/My\\ Drive/deeplearning_project/github.pem\"\n",
        "  DATA_DIR = os.path.join(REPO_DIR, \"repo\")\n",
        "  STORE_DIR =\"/content/drive/My Drive/deeplearning_project/\" \n",
        "  !mkdir ~/.ssh\n",
        "  !cp -u {GIT_PATH} ~/.ssh/\n",
        "  !chmod u=rw,g=,o= ~/.ssh/github.pem\n",
        "  !echo \"{ssh_config}\" > ~/.ssh/config\n",
        "  !chmod u=rw,g=,o= ~/.ssh/config\n",
        "  ! (cd /tmp && git clone git@github.com:davipeag/HeartRateRegression.git)\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "  import sys\n",
        "  sys.path.append(REPO_DIR)\n",
        "\n",
        "def git_pull():\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "\n",
        "git_pull()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=7587fd200a7632417e767c41d443ef36abdda3d1dc2afe2ac642d6a587b595e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Unix-like\n",
            "Mounted at /content/drive\n",
            "Cloning into 'HeartRateRegression'...\n",
            "Warning: Permanently added 'github.com,140.82.112.4' (RSA) to the list of known hosts.\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 548 (delta 99), reused 94 (delta 40), pack-reused 385\u001b[K\n",
            "Receiving objects: 100% (548/548), 87.90 MiB | 35.96 MiB/s, done.\n",
            "Resolving deltas: 100% (326/326), done.\n",
            "Warning: Permanently added the RSA host key for IP address '140.82.114.3' to the list of known hosts.\n",
            "Already up to date.\n",
            "Warning: Permanently added the RSA host key for IP address '140.82.112.3' to the list of known hosts.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCFiZv0xM1pa",
        "outputId": "d58d3094-2b68-4375-d44e-287ba4c4db8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "args = {\n",
        "    'epoch_num': 250,     # Number of epochs.\n",
        "    'lr': 1.0e-3,           # Learning rate.\n",
        "    'weight_decay': 10e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 0,     # Number of workers on data loader.\n",
        "    'batch_size': 128,     # Mini-batch size. 128\n",
        "    'batch_test': 248,     # size of test batch\n",
        "    'window': 15,\n",
        "    'initial_window':5,\n",
        "    'clip_norm': 6.0,     # Upper limit on gradient L2 norm ###\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "SEED = 1234\n",
        "def reset_seeds():\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reset_seeds()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7V97F8pWmvK",
        "outputId": "e4f85ea6-3927-4125-8df8-b4a9fae230d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from data_utils import (PpgDaliaExtractor, FormatPPGDalia)\n",
        "\n",
        "extractor = PpgDaliaExtractor(DATA_DIR)\n",
        "ppg_dalia_formatter = FormatPPGDalia()\n",
        "dfs_train = [ppg_dalia_formatter.transform(extractor.extract_subject(i)) for i in range(1,16)]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdDUK1vToJWo",
        "outputId": "c60be08b-b765-4d2f-f376-d3e42dc2f090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "git_pull()\n",
        "\n",
        "import importlib\n",
        "\n",
        "import PPG\n",
        "importlib.reload(PPG.AttentionDefaults)\n",
        "importlib.reload(PPG)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(PPG.Models)\n",
        "importlib.reload(PPG.TrainerXY)\n",
        "\n",
        "from PPG import FullTrainer"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects:  25% (1/4)   \rUnpacking objects:  50% (2/4)   \rUnpacking objects:  75% (3/4)   \rUnpacking objects: 100% (4/4)   \rUnpacking objects: 100% (4/4), done.\n",
            "From github.com:davipeag/HeartRateRegression\n",
            "   9039be7..b25e066  master     -> origin/master\n",
            "Updating 9039be7..b25e066\n",
            "Fast-forward\n",
            " PPG/FullTrainer.py | 55 \u001b[32m+++++++++++++++++++++++++++\u001b[m\u001b[31m---------------------------\u001b[m\n",
            " 1 file changed, 28 insertions(+), 27 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdPECC3jYpjp"
      },
      "source": [
        "import random \n",
        "options = {\n",
        "  'nfeatures': [4],\n",
        "  'conv_filters': [64, 128, 256],\n",
        "  'nconv_layers': [1, 2, 3, 4],\n",
        "  'conv_dropout': [0, 0.1, 0.25,],\n",
        "  'nenc_layers': [1, 2, 3, 4],\n",
        "  'ndec_layers': [2, 3, 4],\n",
        "  'nhead': [1, 2, 4],\n",
        "  'feedforward_expansion': [1, 2, 4],\n",
        "  'nlin_layers': [2,3,4],\n",
        "  'lin_size': [16, 32, 64],\n",
        "  'lin_dropout': [0, 0.25, 0.5],\n",
        "  'lr': [0.001, 0.0001],\n",
        "  'weight_decay': [0, 0.00001],\n",
        "  'batch_size': [128, 256],\n",
        "  'ts_sub': [0],\n",
        "  'val_sub': [4]\n",
        " }\n",
        "\n",
        "def choose(options):\n",
        "  choice = dict()\n",
        "  for k,v in options.items():\n",
        "    choice[k] = random.choice(v)\n",
        "  return choice\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YFiismva8BL",
        "outputId": "1faf3d03-a069-4f08-bcce-58105671ed59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "while True:\n",
        "  full_trainer = FullTrainer.AttentionFullTrainer(dfs_train, args[\"device\"], 0, 1)\n",
        "  out = full_trainer.train(**choose(options))\n",
        "  print(out[\"args\"], out[\"metric\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/30]: loss_train: 11.529 loss_val 51.567 loss_ts 11.529\n",
            "[3/30]: loss_train: 10.171 loss_val 42.507 loss_ts 10.171\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 10.146 loss_val 21.554 loss_ts 10.146\n",
            "[5/30]: loss_train: 10.351 loss_val 25.997 loss_ts 10.351\n",
            "[6/30]: loss_train: 9.647 loss_val 26.581 loss_ts 9.647\n",
            "[7/30]: loss_train: 9.526 loss_val 30.678 loss_ts 9.526\n",
            "[8/30]: loss_train: 9.714 loss_val 26.867 loss_ts 9.714\n",
            "[9/30]: loss_train: 9.795 loss_val 26.133 loss_ts 9.795\n",
            "best val epoch: 10\n",
            "[10/30]: loss_train: 9.977 loss_val 19.832 loss_ts 9.977\n",
            "best val epoch: 11\n",
            "[11/30]: loss_train: 10.066 loss_val 17.343 loss_ts 10.066\n",
            "best val epoch: 12\n",
            "[12/30]: loss_train: 9.696 loss_val 12.829 loss_ts 9.696\n",
            "[13/30]: loss_train: 8.908 loss_val 27.167 loss_ts 8.908\n",
            "[14/30]: loss_train: 9.318 loss_val 24.269 loss_ts 9.318\n",
            "[15/30]: loss_train: 9.655 loss_val 19.205 loss_ts 9.655\n",
            "[16/30]: loss_train: 9.259 loss_val 24.114 loss_ts 9.259\n",
            "[17/30]: loss_train: 9.757 loss_val 20.354 loss_ts 9.757\n",
            "[18/30]: loss_train: 9.224 loss_val 18.962 loss_ts 9.224\n",
            "[19/30]: loss_train: 9.324 loss_val 23.326 loss_ts 9.324\n",
            "[20/30]: loss_train: 9.161 loss_val 25.402 loss_ts 9.161\n",
            "[21/30]: loss_train: 8.451 loss_val 20.446 loss_ts 8.451\n",
            "[22/30]: loss_train: 8.476 loss_val 14.902 loss_ts 8.476\n",
            "[23/30]: loss_train: 9.370 loss_val 22.943 loss_ts 9.370\n",
            "[24/30]: loss_train: 9.097 loss_val 23.203 loss_ts 9.097\n",
            "[25/30]: loss_train: 8.909 loss_val 26.240 loss_ts 8.909\n",
            "[26/30]: loss_train: 8.941 loss_val 18.629 loss_ts 8.941\n",
            "[27/30]: loss_train: 9.180 loss_val 24.269 loss_ts 9.180\n",
            "[28/30]: loss_train: 8.711 loss_val 23.988 loss_ts 8.711\n",
            "[29/30]: loss_train: 8.984 loss_val 27.483 loss_ts 8.984\n",
            "[30/30]: loss_train: 10.742 loss_val 45.124 loss_ts 10.742\n",
            "Final: 9.696052551269531\n",
            "{'val_sub': 4, 'ts_sub': 0, 'batch_size': 256, 'weight_decay': 0, 'lr': 0.001, 'lin_dropout': 0.25, 'lin_size': 64, 'nlin_layers': 1, 'feedforward_expansion': 1, 'nhead': 4, 'ndec_layers': 1, 'nenc_layers': 1, 'conv_dropout': 0, 'nconv_layers': 2, 'conv_filters': 128, 'nfeatures': 4} 9.696053\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 11.408 loss_val 27.568 loss_ts 11.408\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 9.567 loss_val 20.237 loss_ts 9.567\n",
            "best val epoch: 3\n",
            "[3/30]: loss_train: 9.582 loss_val 17.546 loss_ts 9.582\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 9.627 loss_val 15.411 loss_ts 9.627\n",
            "[5/30]: loss_train: 8.895 loss_val 21.551 loss_ts 8.895\n",
            "best val epoch: 6\n",
            "[6/30]: loss_train: 9.139 loss_val 14.847 loss_ts 9.139\n",
            "[7/30]: loss_train: 9.185 loss_val 18.085 loss_ts 9.185\n",
            "[8/30]: loss_train: 8.850 loss_val 15.392 loss_ts 8.850\n",
            "[9/30]: loss_train: 8.150 loss_val 17.013 loss_ts 8.150\n",
            "best val epoch: 10\n",
            "[10/30]: loss_train: 8.114 loss_val 9.301 loss_ts 8.114\n",
            "[11/30]: loss_train: 7.533 loss_val 12.501 loss_ts 7.533\n",
            "[12/30]: loss_train: 8.801 loss_val 13.064 loss_ts 8.801\n",
            "[13/30]: loss_train: 8.368 loss_val 13.362 loss_ts 8.368\n",
            "[14/30]: loss_train: 8.001 loss_val 19.905 loss_ts 8.001\n",
            "[15/30]: loss_train: 7.760 loss_val 13.446 loss_ts 7.760\n",
            "[16/30]: loss_train: 7.659 loss_val 17.886 loss_ts 7.659\n",
            "[17/30]: loss_train: 7.089 loss_val 16.174 loss_ts 7.089\n",
            "[18/30]: loss_train: 6.774 loss_val 15.296 loss_ts 6.774\n",
            "[19/30]: loss_train: 6.767 loss_val 15.131 loss_ts 6.767\n",
            "[20/30]: loss_train: 6.990 loss_val 14.758 loss_ts 6.990\n",
            "[21/30]: loss_train: 6.323 loss_val 17.154 loss_ts 6.323\n",
            "[22/30]: loss_train: 6.671 loss_val 12.599 loss_ts 6.671\n",
            "[23/30]: loss_train: 8.325 loss_val 14.912 loss_ts 8.325\n",
            "[24/30]: loss_train: 6.142 loss_val 11.631 loss_ts 6.142\n",
            "[25/30]: loss_train: 6.389 loss_val 13.433 loss_ts 6.389\n",
            "[26/30]: loss_train: 6.046 loss_val 16.524 loss_ts 6.046\n",
            "[27/30]: loss_train: 6.149 loss_val 17.924 loss_ts 6.149\n",
            "[28/30]: loss_train: 5.964 loss_val 10.383 loss_ts 5.964\n",
            "[29/30]: loss_train: 6.578 loss_val 12.784 loss_ts 6.578\n",
            "[30/30]: loss_train: 7.161 loss_val 12.187 loss_ts 7.161\n",
            "Final: 8.113631248474121\n",
            "{'val_sub': 4, 'ts_sub': 0, 'batch_size': 256, 'weight_decay': 0, 'lr': 0.0001, 'lin_dropout': 0.25, 'lin_size': 32, 'nlin_layers': 2, 'feedforward_expansion': 4, 'nhead': 1, 'ndec_layers': 4, 'nenc_layers': 2, 'conv_dropout': 0, 'nconv_layers': 4, 'conv_filters': 128, 'nfeatures': 4} 8.113631\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 13.596 loss_val 42.236 loss_ts 13.596\n",
            "[2/30]: loss_train: 13.891 loss_val 42.312 loss_ts 13.891\n",
            "[3/30]: loss_train: 13.788 loss_val 42.429 loss_ts 13.788\n",
            "[4/30]: loss_train: 14.027 loss_val 42.748 loss_ts 14.027\n",
            "best val epoch: 5\n",
            "[5/30]: loss_train: 13.889 loss_val 41.352 loss_ts 13.889\n",
            "best val epoch: 6\n",
            "[6/30]: loss_train: 13.911 loss_val 39.897 loss_ts 13.911\n",
            "best val epoch: 7\n",
            "[7/30]: loss_train: 14.007 loss_val 39.571 loss_ts 14.007\n",
            "best val epoch: 8\n",
            "[8/30]: loss_train: 13.000 loss_val 37.180 loss_ts 13.000\n",
            "best val epoch: 9\n",
            "[9/30]: loss_train: 13.227 loss_val 33.850 loss_ts 13.227\n",
            "best val epoch: 10\n",
            "[10/30]: loss_train: 13.146 loss_val 33.141 loss_ts 13.146\n",
            "[11/30]: loss_train: 12.605 loss_val 33.370 loss_ts 12.605\n",
            "best val epoch: 12\n",
            "[12/30]: loss_train: 12.911 loss_val 32.648 loss_ts 12.911\n",
            "[13/30]: loss_train: 12.974 loss_val 33.350 loss_ts 12.974\n",
            "best val epoch: 14\n",
            "[14/30]: loss_train: 13.839 loss_val 32.333 loss_ts 13.839\n",
            "best val epoch: 15\n",
            "[15/30]: loss_train: 13.476 loss_val 32.083 loss_ts 13.476\n",
            "[16/30]: loss_train: 12.905 loss_val 34.518 loss_ts 12.905\n",
            "[17/30]: loss_train: 12.633 loss_val 32.538 loss_ts 12.633\n",
            "[18/30]: loss_train: 12.211 loss_val 32.452 loss_ts 12.211\n",
            "[19/30]: loss_train: 12.161 loss_val 32.584 loss_ts 12.161\n",
            "[20/30]: loss_train: 11.720 loss_val 32.102 loss_ts 11.720\n",
            "best val epoch: 21\n",
            "[21/30]: loss_train: 12.664 loss_val 31.214 loss_ts 12.664\n",
            "best val epoch: 22\n",
            "[22/30]: loss_train: 12.737 loss_val 31.173 loss_ts 12.737\n",
            "[23/30]: loss_train: 11.752 loss_val 31.373 loss_ts 11.752\n",
            "[24/30]: loss_train: 12.024 loss_val 31.498 loss_ts 12.024\n",
            "[25/30]: loss_train: 11.643 loss_val 32.888 loss_ts 11.643\n",
            "best val epoch: 26\n",
            "[26/30]: loss_train: 12.501 loss_val 29.392 loss_ts 12.501\n",
            "[27/30]: loss_train: 12.299 loss_val 30.526 loss_ts 12.299\n",
            "[28/30]: loss_train: 11.674 loss_val 31.030 loss_ts 11.674\n",
            "best val epoch: 29\n",
            "[29/30]: loss_train: 12.264 loss_val 28.468 loss_ts 12.264\n",
            "[30/30]: loss_train: 12.127 loss_val 31.262 loss_ts 12.127\n",
            "Final: 12.2643461227417\n",
            "{'val_sub': 4, 'ts_sub': 0, 'batch_size': 64, 'weight_decay': 0.0001, 'lr': 0.001, 'lin_dropout': 0, 'lin_size': 16, 'nlin_layers': 2, 'feedforward_expansion': 2, 'nhead': 4, 'ndec_layers': 1, 'nenc_layers': 2, 'conv_dropout': 0, 'nconv_layers': 0, 'conv_filters': 128, 'nfeatures': 4} 12.264346\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 12.184 loss_val 41.221 loss_ts 12.184\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 10.336 loss_val 26.025 loss_ts 10.336\n",
            "[3/30]: loss_train: 9.323 loss_val 26.110 loss_ts 9.323\n",
            "[4/30]: loss_train: 9.352 loss_val 28.279 loss_ts 9.352\n",
            "best val epoch: 5\n",
            "[5/30]: loss_train: 9.790 loss_val 19.595 loss_ts 9.790\n",
            "best val epoch: 6\n",
            "[6/30]: loss_train: 9.642 loss_val 18.629 loss_ts 9.642\n",
            "[7/30]: loss_train: 9.567 loss_val 20.818 loss_ts 9.567\n",
            "[8/30]: loss_train: 9.357 loss_val 21.270 loss_ts 9.357\n",
            "best val epoch: 9\n",
            "[9/30]: loss_train: 9.057 loss_val 16.338 loss_ts 9.057\n",
            "[10/30]: loss_train: 9.496 loss_val 17.754 loss_ts 9.496\n",
            "best val epoch: 11\n",
            "[11/30]: loss_train: 9.941 loss_val 16.184 loss_ts 9.941\n",
            "[12/30]: loss_train: 9.093 loss_val 17.554 loss_ts 9.093\n",
            "best val epoch: 13\n",
            "[13/30]: loss_train: 9.857 loss_val 13.784 loss_ts 9.857\n",
            "[14/30]: loss_train: 9.135 loss_val 19.380 loss_ts 9.135\n",
            "[15/30]: loss_train: 8.766 loss_val 18.132 loss_ts 8.766\n",
            "[16/30]: loss_train: 8.557 loss_val 16.964 loss_ts 8.557\n",
            "[17/30]: loss_train: 8.714 loss_val 17.749 loss_ts 8.714\n",
            "[18/30]: loss_train: 9.052 loss_val 21.300 loss_ts 9.052\n",
            "[19/30]: loss_train: 9.012 loss_val 16.967 loss_ts 9.012\n",
            "[20/30]: loss_train: 8.381 loss_val 20.639 loss_ts 8.381\n",
            "[21/30]: loss_train: 8.498 loss_val 15.950 loss_ts 8.498\n",
            "[22/30]: loss_train: 7.833 loss_val 14.856 loss_ts 7.833\n",
            "[23/30]: loss_train: 7.979 loss_val 17.490 loss_ts 7.979\n",
            "[24/30]: loss_train: 7.790 loss_val 16.254 loss_ts 7.790\n",
            "[25/30]: loss_train: 7.791 loss_val 16.576 loss_ts 7.791\n",
            "[26/30]: loss_train: 6.882 loss_val 17.736 loss_ts 6.882\n",
            "[27/30]: loss_train: 7.110 loss_val 14.699 loss_ts 7.110\n",
            "[28/30]: loss_train: 7.484 loss_val 16.867 loss_ts 7.484\n",
            "[29/30]: loss_train: 7.759 loss_val 14.930 loss_ts 7.759\n",
            "[30/30]: loss_train: 6.068 loss_val 17.118 loss_ts 6.068\n",
            "Final: 9.856819152832031\n",
            "{'val_sub': 4, 'ts_sub': 0, 'batch_size': 64, 'weight_decay': 0, 'lr': 0.0001, 'lin_dropout': 0.25, 'lin_size': 16, 'nlin_layers': 2, 'feedforward_expansion': 2, 'nhead': 4, 'ndec_layers': 1, 'nenc_layers': 4, 'conv_dropout': 0, 'nconv_layers': 2, 'conv_filters': 32, 'nfeatures': 4} 9.856819\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 11.611 loss_val 44.125 loss_ts 11.611\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 12.761 loss_val 37.954 loss_ts 12.761\n",
            "best val epoch: 3\n",
            "[3/30]: loss_train: 9.936 loss_val 25.569 loss_ts 9.936\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 9.583 loss_val 22.470 loss_ts 9.583\n",
            "best val epoch: 5\n",
            "[5/30]: loss_train: 9.963 loss_val 21.965 loss_ts 9.963\n",
            "best val epoch: 6\n",
            "[6/30]: loss_train: 9.629 loss_val 19.089 loss_ts 9.629\n",
            "[7/30]: loss_train: 9.142 loss_val 21.231 loss_ts 9.142\n",
            "best val epoch: 8\n",
            "[8/30]: loss_train: 10.051 loss_val 15.259 loss_ts 10.051\n",
            "[9/30]: loss_train: 8.993 loss_val 18.596 loss_ts 8.993\n",
            "[10/30]: loss_train: 9.289 loss_val 15.982 loss_ts 9.289\n",
            "[11/30]: loss_train: 10.091 loss_val 16.027 loss_ts 10.091\n",
            "best val epoch: 12\n",
            "[12/30]: loss_train: 9.431 loss_val 12.723 loss_ts 9.431\n",
            "[13/30]: loss_train: 8.996 loss_val 20.083 loss_ts 8.996\n",
            "[14/30]: loss_train: 9.588 loss_val 15.919 loss_ts 9.588\n",
            "[15/30]: loss_train: 8.833 loss_val 16.953 loss_ts 8.833\n",
            "[16/30]: loss_train: 9.578 loss_val 17.689 loss_ts 9.578\n",
            "[17/30]: loss_train: 8.864 loss_val 16.718 loss_ts 8.864\n",
            "[18/30]: loss_train: 9.079 loss_val 15.623 loss_ts 9.079\n",
            "[19/30]: loss_train: 8.357 loss_val 17.598 loss_ts 8.357\n",
            "[20/30]: loss_train: 9.448 loss_val 15.010 loss_ts 9.448\n",
            "[21/30]: loss_train: 8.847 loss_val 14.393 loss_ts 8.847\n",
            "[22/30]: loss_train: 8.859 loss_val 19.201 loss_ts 8.859\n",
            "[23/30]: loss_train: 8.195 loss_val 15.560 loss_ts 8.195\n",
            "[24/30]: loss_train: 8.270 loss_val 15.285 loss_ts 8.270\n",
            "[25/30]: loss_train: 8.177 loss_val 19.440 loss_ts 8.177\n",
            "best val epoch: 26\n",
            "[26/30]: loss_train: 8.005 loss_val 12.086 loss_ts 8.005\n",
            "[27/30]: loss_train: 7.997 loss_val 14.624 loss_ts 7.997\n",
            "[28/30]: loss_train: 7.312 loss_val 16.470 loss_ts 7.312\n",
            "[29/30]: loss_train: 7.461 loss_val 16.420 loss_ts 7.461\n",
            "[30/30]: loss_train: 8.790 loss_val 15.191 loss_ts 8.790\n",
            "Final: 8.005270004272461\n",
            "{'val_sub': 4, 'ts_sub': 0, 'batch_size': 128, 'weight_decay': 0, 'lr': 0.0001, 'lin_dropout': 0.25, 'lin_size': 64, 'nlin_layers': 2, 'feedforward_expansion': 2, 'nhead': 2, 'ndec_layers': 2, 'nenc_layers': 4, 'conv_dropout': 0, 'nconv_layers': 2, 'conv_filters': 32, 'nfeatures': 4} 8.00527\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 11.064 loss_val 43.245 loss_ts 11.064\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 9.114 loss_val 24.874 loss_ts 9.114\n",
            "best val epoch: 3\n",
            "[3/30]: loss_train: 11.392 loss_val 17.545 loss_ts 11.392\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 11.519 loss_val 15.435 loss_ts 11.519\n",
            "best val epoch: 5\n",
            "[5/30]: loss_train: 11.832 loss_val 15.033 loss_ts 11.832\n",
            "[6/30]: loss_train: 9.271 loss_val 20.177 loss_ts 9.271\n",
            "[7/30]: loss_train: 10.654 loss_val 16.450 loss_ts 10.654\n",
            "[8/30]: loss_train: 9.834 loss_val 16.940 loss_ts 9.834\n",
            "[9/30]: loss_train: 9.870 loss_val 16.493 loss_ts 9.870\n",
            "[10/30]: loss_train: 10.698 loss_val 15.517 loss_ts 10.698\n",
            "[11/30]: loss_train: 9.161 loss_val 17.638 loss_ts 9.161\n",
            "[12/30]: loss_train: 10.199 loss_val 15.137 loss_ts 10.199\n",
            "[13/30]: loss_train: 8.890 loss_val 18.479 loss_ts 8.890\n",
            "[14/30]: loss_train: 9.064 loss_val 20.627 loss_ts 9.064\n",
            "best val epoch: 15\n",
            "[15/30]: loss_train: 10.724 loss_val 14.213 loss_ts 10.724\n",
            "[16/30]: loss_train: 8.959 loss_val 18.033 loss_ts 8.959\n",
            "[17/30]: loss_train: 8.621 loss_val 15.620 loss_ts 8.621\n",
            "[18/30]: loss_train: 8.988 loss_val 15.446 loss_ts 8.988\n",
            "[19/30]: loss_train: 9.651 loss_val 15.755 loss_ts 9.651\n",
            "[20/30]: loss_train: 8.970 loss_val 16.466 loss_ts 8.970\n",
            "[21/30]: loss_train: 9.666 loss_val 14.730 loss_ts 9.666\n",
            "[22/30]: loss_train: 9.017 loss_val 16.295 loss_ts 9.017\n",
            "best val epoch: 23\n",
            "[23/30]: loss_train: 9.354 loss_val 13.697 loss_ts 9.354\n",
            "[24/30]: loss_train: 8.402 loss_val 17.117 loss_ts 8.402\n",
            "[25/30]: loss_train: 8.711 loss_val 16.593 loss_ts 8.711\n",
            "[26/30]: loss_train: 9.177 loss_val 15.865 loss_ts 9.177\n",
            "[27/30]: loss_train: 8.480 loss_val 15.354 loss_ts 8.480\n",
            "[28/30]: loss_train: 8.846 loss_val 15.980 loss_ts 8.846\n",
            "[29/30]: loss_train: 9.128 loss_val 15.018 loss_ts 9.128\n",
            "[30/30]: loss_train: 8.688 loss_val 13.726 loss_ts 8.688\n",
            "Final: 9.354283332824707\n",
            "{'val_sub': 4, 'ts_sub': 0, 'batch_size': 128, 'weight_decay': 0, 'lr': 0.0001, 'lin_dropout': 0.25, 'lin_size': 32, 'nlin_layers': 1, 'feedforward_expansion': 1, 'nhead': 2, 'ndec_layers': 2, 'nenc_layers': 1, 'conv_dropout': 0, 'nconv_layers': 2, 'conv_filters': 64, 'nfeatures': 4} 9.354283\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 11.697 loss_val 42.467 loss_ts 11.697\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 11.078 loss_val 25.912 loss_ts 11.078\n",
            "best val epoch: 3\n",
            "[3/30]: loss_train: 11.148 loss_val 23.384 loss_ts 11.148\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 11.007 loss_val 21.603 loss_ts 11.007\n",
            "[5/30]: loss_train: 10.132 loss_val 21.964 loss_ts 10.132\n",
            "best val epoch: 6\n",
            "[6/30]: loss_train: 10.495 loss_val 19.855 loss_ts 10.495\n",
            "best val epoch: 7\n",
            "[7/30]: loss_train: 11.669 loss_val 19.367 loss_ts 11.669\n",
            "best val epoch: 8\n",
            "[8/30]: loss_train: 10.431 loss_val 18.712 loss_ts 10.431\n",
            "best val epoch: 9\n",
            "[9/30]: loss_train: 10.775 loss_val 17.157 loss_ts 10.775\n",
            "[10/30]: loss_train: 10.116 loss_val 19.186 loss_ts 10.116\n",
            "[11/30]: loss_train: 10.521 loss_val 17.818 loss_ts 10.521\n",
            "best val epoch: 12\n",
            "[12/30]: loss_train: 10.914 loss_val 16.042 loss_ts 10.914\n",
            "best val epoch: 13\n",
            "[13/30]: loss_train: 10.864 loss_val 15.802 loss_ts 10.864\n",
            "[14/30]: loss_train: 10.342 loss_val 18.048 loss_ts 10.342\n",
            "[15/30]: loss_train: 10.738 loss_val 16.156 loss_ts 10.738\n",
            "best val epoch: 16\n",
            "[16/30]: loss_train: 10.309 loss_val 14.937 loss_ts 10.309\n",
            "[17/30]: loss_train: 10.301 loss_val 15.079 loss_ts 10.301\n",
            "best val epoch: 18\n",
            "[18/30]: loss_train: 10.605 loss_val 13.947 loss_ts 10.605\n",
            "[19/30]: loss_train: 10.142 loss_val 14.591 loss_ts 10.142\n",
            "[20/30]: loss_train: 10.480 loss_val 15.608 loss_ts 10.480\n",
            "[21/30]: loss_train: 9.774 loss_val 15.234 loss_ts 9.774\n",
            "best val epoch: 22\n",
            "[22/30]: loss_train: 10.398 loss_val 13.884 loss_ts 10.398\n",
            "best val epoch: 23\n",
            "[23/30]: loss_train: 10.157 loss_val 13.072 loss_ts 10.157\n",
            "[24/30]: loss_train: 9.673 loss_val 13.769 loss_ts 9.673\n",
            "[25/30]: loss_train: 9.534 loss_val 13.648 loss_ts 9.534\n",
            "[26/30]: loss_train: 10.375 loss_val 14.085 loss_ts 10.375\n",
            "best val epoch: 27\n",
            "[27/30]: loss_train: 10.247 loss_val 12.008 loss_ts 10.247\n",
            "best val epoch: 28\n",
            "[28/30]: loss_train: 9.825 loss_val 11.824 loss_ts 9.825\n",
            "[29/30]: loss_train: 9.827 loss_val 12.805 loss_ts 9.827\n",
            "best val epoch: 30\n",
            "[30/30]: loss_train: 9.877 loss_val 10.853 loss_ts 9.877\n",
            "Final: 9.876607894897461\n",
            "{'val_sub': 4, 'ts_sub': 0, 'batch_size': 64, 'weight_decay': 0, 'lr': 0.0001, 'lin_dropout': 0, 'lin_size': 16, 'nlin_layers': 2, 'feedforward_expansion': 1, 'nhead': 1, 'ndec_layers': 4, 'nenc_layers': 2, 'conv_dropout': 0.25, 'nconv_layers': 4, 'conv_filters': 32, 'nfeatures': 4} 9.876608\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 12.459 loss_val 43.911 loss_ts 12.459\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 12.934 loss_val 42.208 loss_ts 12.934\n",
            "[3/30]: loss_train: 12.664 loss_val 42.302 loss_ts 12.664\n",
            "[4/30]: loss_train: 11.693 loss_val 43.728 loss_ts 11.693\n",
            "[5/30]: loss_train: 11.070 loss_val 44.051 loss_ts 11.070\n",
            "[6/30]: loss_train: 10.669 loss_val 43.743 loss_ts 10.669\n",
            "[7/30]: loss_train: 10.379 loss_val 43.621 loss_ts 10.379\n",
            "[8/30]: loss_train: 10.411 loss_val 42.882 loss_ts 10.411\n",
            "best val epoch: 9\n",
            "[9/30]: loss_train: 10.469 loss_val 41.820 loss_ts 10.469\n",
            "[10/30]: loss_train: 10.394 loss_val 41.973 loss_ts 10.394\n",
            "best val epoch: 11\n",
            "[11/30]: loss_train: 10.410 loss_val 41.102 loss_ts 10.410\n",
            "best val epoch: 12\n",
            "[12/30]: loss_train: 10.530 loss_val 40.517 loss_ts 10.530\n",
            "best val epoch: 13\n",
            "[13/30]: loss_train: 10.622 loss_val 39.720 loss_ts 10.622\n",
            "best val epoch: 14\n",
            "[14/30]: loss_train: 10.554 loss_val 39.469 loss_ts 10.554\n",
            "[15/30]: loss_train: 10.184 loss_val 40.258 loss_ts 10.184\n",
            "[16/30]: loss_train: 10.264 loss_val 40.050 loss_ts 10.264\n",
            "best val epoch: 17\n",
            "[17/30]: loss_train: 10.247 loss_val 39.389 loss_ts 10.247\n",
            "[18/30]: loss_train: 10.177 loss_val 39.435 loss_ts 10.177\n",
            "[19/30]: loss_train: 10.050 loss_val 39.710 loss_ts 10.050\n",
            "[20/30]: loss_train: 10.084 loss_val 39.604 loss_ts 10.084\n",
            "[21/30]: loss_train: 10.226 loss_val 39.421 loss_ts 10.226\n",
            "[22/30]: loss_train: 10.128 loss_val 39.497 loss_ts 10.128\n",
            "[23/30]: loss_train: 10.168 loss_val 39.669 loss_ts 10.168\n",
            "best val epoch: 24\n",
            "[24/30]: loss_train: 10.207 loss_val 39.104 loss_ts 10.207\n",
            "best val epoch: 25\n",
            "[25/30]: loss_train: 10.302 loss_val 38.449 loss_ts 10.302\n",
            "[26/30]: loss_train: 10.278 loss_val 38.549 loss_ts 10.278\n",
            "best val epoch: 27\n",
            "[27/30]: loss_train: 10.251 loss_val 38.444 loss_ts 10.251\n",
            "[28/30]: loss_train: 9.988 loss_val 39.849 loss_ts 9.988\n",
            "[29/30]: loss_train: 10.203 loss_val 38.776 loss_ts 10.203\n",
            "[30/30]: loss_train: 10.335 loss_val 38.718 loss_ts 10.335\n",
            "Final: 10.251151084899902\n",
            "{'val_sub': 4, 'ts_sub': 0, 'batch_size': 256, 'weight_decay': 0.0001, 'lr': 0.001, 'lin_dropout': 0.25, 'lin_size': 16, 'nlin_layers': 1, 'feedforward_expansion': 2, 'nhead': 4, 'ndec_layers': 1, 'nenc_layers': 2, 'conv_dropout': 0.5, 'nconv_layers': 0, 'conv_filters': 64, 'nfeatures': 4} 10.251151\n",
            "[1/30]: loss_train: 11.505 loss_val 51.688 loss_ts 11.505\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 13.773 loss_val 45.078 loss_ts 13.773\n",
            "[3/30]: loss_train: 12.167 loss_val 49.118 loss_ts 12.167\n",
            "[4/30]: loss_train: 10.207 loss_val 47.882 loss_ts 10.207\n",
            "[5/30]: loss_train: 10.284 loss_val 48.825 loss_ts 10.284\n",
            "[6/30]: loss_train: 10.945 loss_val 51.031 loss_ts 10.945\n",
            "best val epoch: 7\n",
            "[7/30]: loss_train: 13.087 loss_val 44.083 loss_ts 13.087\n",
            "[8/30]: loss_train: 12.282 loss_val 44.300 loss_ts 12.282\n",
            "[9/30]: loss_train: 10.636 loss_val 48.806 loss_ts 10.636\n",
            "[10/30]: loss_train: 12.825 loss_val 44.350 loss_ts 12.825\n",
            "[11/30]: loss_train: 10.919 loss_val 47.817 loss_ts 10.919\n",
            "[12/30]: loss_train: 11.064 loss_val 50.787 loss_ts 11.064\n",
            "[13/30]: loss_train: 13.448 loss_val 45.218 loss_ts 13.448\n",
            "[14/30]: loss_train: 11.401 loss_val 49.133 loss_ts 11.401\n",
            "[15/30]: loss_train: 11.269 loss_val 50.926 loss_ts 11.269\n",
            "[16/30]: loss_train: 11.272 loss_val 50.894 loss_ts 11.272\n",
            "[17/30]: loss_train: 11.527 loss_val 51.052 loss_ts 11.527\n",
            "[18/30]: loss_train: 11.390 loss_val 50.964 loss_ts 11.390\n",
            "[19/30]: loss_train: 11.571 loss_val 51.119 loss_ts 11.571\n",
            "[20/30]: loss_train: 11.578 loss_val 51.091 loss_ts 11.578\n",
            "[21/30]: loss_train: 11.384 loss_val 50.931 loss_ts 11.384\n",
            "[22/30]: loss_train: 11.546 loss_val 51.031 loss_ts 11.546\n",
            "[23/30]: loss_train: 11.337 loss_val 50.947 loss_ts 11.337\n",
            "[24/30]: loss_train: 11.252 loss_val 50.867 loss_ts 11.252\n",
            "[25/30]: loss_train: 11.256 loss_val 47.388 loss_ts 11.256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFOG4ar9WYpt",
        "outputId": "e8eeb337-f02c-4288-e8c3-c2be7d53a570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "full_trainer = FullTrainer.AttentionFullTrainer(dfs_train, args[\"device\"], 0, 1)\n",
        "\n",
        "full_trainer.train()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best val epoch: 1\n",
            "[1/30]: loss_train: 17.761 loss_val 37.989 loss_ts 17.761\n",
            "[2/30]: loss_train: 13.198 loss_val 41.495 loss_ts 13.198\n",
            "[3/30]: loss_train: 14.183 loss_val 44.654 loss_ts 14.183\n",
            "[4/30]: loss_train: 13.810 loss_val 45.692 loss_ts 13.810\n",
            "[5/30]: loss_train: 13.165 loss_val 48.072 loss_ts 13.165\n",
            "[6/30]: loss_train: 14.161 loss_val 41.123 loss_ts 14.161\n",
            "best val epoch: 7\n",
            "[7/30]: loss_train: 13.948 loss_val 37.674 loss_ts 13.948\n",
            "[8/30]: loss_train: 14.359 loss_val 42.283 loss_ts 14.359\n",
            "[9/30]: loss_train: 13.799 loss_val 39.775 loss_ts 13.799\n",
            "[10/30]: loss_train: 13.806 loss_val 40.962 loss_ts 13.806\n",
            "best val epoch: 11\n",
            "[11/30]: loss_train: 13.495 loss_val 37.275 loss_ts 13.495\n",
            "[12/30]: loss_train: 12.646 loss_val 39.879 loss_ts 12.646\n",
            "[13/30]: loss_train: 12.792 loss_val 40.446 loss_ts 12.792\n",
            "[14/30]: loss_train: 13.287 loss_val 48.494 loss_ts 13.287\n",
            "[15/30]: loss_train: 14.351 loss_val 45.145 loss_ts 14.351\n",
            "[16/30]: loss_train: 14.426 loss_val 44.753 loss_ts 14.426\n",
            "[17/30]: loss_train: 14.319 loss_val 39.213 loss_ts 14.319\n",
            "[18/30]: loss_train: 14.116 loss_val 40.425 loss_ts 14.116\n",
            "[19/30]: loss_train: 14.833 loss_val 38.680 loss_ts 14.833\n",
            "[20/30]: loss_train: 12.721 loss_val 40.922 loss_ts 12.721\n",
            "[21/30]: loss_train: 13.698 loss_val 40.192 loss_ts 13.698\n",
            "[22/30]: loss_train: 13.255 loss_val 40.970 loss_ts 13.255\n",
            "[23/30]: loss_train: 13.329 loss_val 40.884 loss_ts 13.329\n",
            "[24/30]: loss_train: 13.571 loss_val 42.128 loss_ts 13.571\n",
            "[25/30]: loss_train: 14.248 loss_val 40.825 loss_ts 14.248\n",
            "[26/30]: loss_train: 15.545 loss_val 42.472 loss_ts 15.545\n",
            "[27/30]: loss_train: 13.342 loss_val 44.750 loss_ts 13.342\n",
            "[28/30]: loss_train: 13.837 loss_val 45.402 loss_ts 13.837\n",
            "[29/30]: loss_train: 13.683 loss_val 46.327 loss_ts 13.683\n",
            "[30/30]: loss_train: 14.671 loss_val 43.511 loss_ts 14.671\n",
            "Final: 13.495309829711914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'args': {'batch_size': 128,\n",
              "  'conv_dropout': 0.5,\n",
              "  'conv_filters': 64,\n",
              "  'feedforward_expansion': 2,\n",
              "  'lin_dropout': 0,\n",
              "  'lin_size': 32,\n",
              "  'lr': 0.001,\n",
              "  'nconv_layers': 4,\n",
              "  'ndec_layers': 2,\n",
              "  'nenc_layers': 2,\n",
              "  'nfeatures': 4,\n",
              "  'nhead': 4,\n",
              "  'nlin_layers': 2,\n",
              "  'ts_sub': 0,\n",
              "  'val_sub': 4,\n",
              "  'weight_decay': 0.0001},\n",
              " 'metric': 13.49531,\n",
              " 'predictions': tensor([[83.5864],\n",
              "         [99.5204],\n",
              "         [99.5203],\n",
              "         ...,\n",
              "         [74.7939],\n",
              "         [74.7939],\n",
              "         [99.5206]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYVLTnMGXYGp"
      },
      "source": [
        "from PPG import UtilitiesDataXY \n",
        "\n",
        "\n",
        "transformers = PPG.AttentionDefaults.get_preprocessing_transformer()\n",
        "make_loaders = UtilitiesDataXY.DataLoaderFactory(transformers, dfs_train).make_loaders\n",
        "\n",
        "loader_tr, loader_val, loader_ts = make_loaders(ts_sub=0, val_sub=1)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Vr-YDyoGfH"
      },
      "source": [
        "from PPG.Models import SnippetConvolutionalTransformer\n",
        "\n",
        "net = SnippetConvolutionalTransformer().to(args[\"device\"])\n",
        "\n",
        "# x,y = next(iter(loader_tr))\n",
        "\n",
        "# p = net(x)\n",
        "\n",
        "criterion = nn.MSELoss().to(args[\"device\"])# nn.L1Loss().to(args[\"device\"]) #nn.CrossEntropyLoss().to(args[\"device\"])\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=args[\"lr\"],\n",
        "                             weight_decay=args[\"weight_decay\"])\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ktvTpTaOcy"
      },
      "source": [
        "from PPG.TrainerXY import (EpochTrainerXY, MetricsComputerXY, TrainHelperXY)\n",
        "from preprocessing_utils import ZTransformer2\n",
        "\n",
        "epoch_trainer = EpochTrainerXY(net, optimizer, criterion, args[\"device\"])\n",
        "ztransformer = ZTransformer2(['heart_rate', 'wrist-ACC-0', 'wrist-ACC-1', 'wrist-ACC-2',\n",
        "              'wrist-BVP-0', 'wrist-EDA-0', 'wrist-TEMP-0', 'chest-ACC-0',\n",
        "              'chest-ACC-1', 'chest-ACC-2', 'chest-Resp-0'])\n",
        "metrics_comuter = MetricsComputerXY(ztransformer)\n",
        "\n",
        "train_helper = TrainHelperXY(epoch_trainer, loader_tr, loader_val, loader_ts, metrics_comuter.mae)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lyVMlfzkaFA",
        "outputId": "68ff27ff-6492-4fa9-a186-ee5aff27b8a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "train_helper.train(30)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/30]: loss_train: 17.742 loss_val 11.444 loss_ts 17.742\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 12.161 loss_val 6.700 loss_ts 12.161\n",
            "[3/30]: loss_train: 12.527 loss_val 7.174 loss_ts 12.527\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 11.101 loss_val 6.087 loss_ts 11.101\n",
            "best val epoch: 5\n",
            "[5/30]: loss_train: 11.757 loss_val 5.934 loss_ts 11.757\n",
            "[6/30]: loss_train: 9.287 loss_val 6.133 loss_ts 9.287\n",
            "[7/30]: loss_train: 8.845 loss_val 6.866 loss_ts 8.845\n",
            "[8/30]: loss_train: 9.448 loss_val 5.995 loss_ts 9.448\n",
            "[9/30]: loss_train: 7.419 loss_val 7.544 loss_ts 7.419\n",
            "[10/30]: loss_train: 8.696 loss_val 6.355 loss_ts 8.696\n",
            "[11/30]: loss_train: 9.077 loss_val 6.228 loss_ts 9.077\n",
            "[12/30]: loss_train: 8.272 loss_val 6.616 loss_ts 8.272\n",
            "[13/30]: loss_train: 8.692 loss_val 6.357 loss_ts 8.692\n",
            "[14/30]: loss_train: 8.125 loss_val 6.584 loss_ts 8.125\n",
            "best val epoch: 15\n",
            "[15/30]: loss_train: 7.320 loss_val 5.822 loss_ts 7.320\n",
            "[16/30]: loss_train: 7.705 loss_val 5.927 loss_ts 7.705\n",
            "[17/30]: loss_train: 7.387 loss_val 5.898 loss_ts 7.387\n",
            "[18/30]: loss_train: 8.434 loss_val 6.686 loss_ts 8.434\n",
            "[19/30]: loss_train: 9.108 loss_val 7.179 loss_ts 9.108\n",
            "[20/30]: loss_train: 7.359 loss_val 5.984 loss_ts 7.359\n",
            "best val epoch: 21\n",
            "[21/30]: loss_train: 6.791 loss_val 5.261 loss_ts 6.791\n",
            "[22/30]: loss_train: 7.194 loss_val 5.859 loss_ts 7.194\n",
            "[23/30]: loss_train: 7.019 loss_val 5.638 loss_ts 7.019\n",
            "[24/30]: loss_train: 6.342 loss_val 6.295 loss_ts 6.342\n",
            "[25/30]: loss_train: 9.890 loss_val 6.797 loss_ts 9.890\n",
            "[26/30]: loss_train: 7.621 loss_val 5.941 loss_ts 7.621\n",
            "[27/30]: loss_train: 6.376 loss_val 5.286 loss_ts 6.376\n",
            "[28/30]: loss_train: 7.135 loss_val 5.798 loss_ts 7.135\n",
            "[29/30]: loss_train: 6.914 loss_val 5.526 loss_ts 6.914\n",
            "[30/30]: loss_train: 6.601 loss_val 5.555 loss_ts 6.601\n",
            "Final: 6.7914910316467285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.791491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0tSXLWPuCQ"
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    }
  ]
}