{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "ppg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksg4pPcCWcJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47be89d7-b04a-4e09-aedc-2209fb4f1356"
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "ssh_config = \"\"\"\n",
        "Host github.com\n",
        "  IdentityFile ~/.ssh/github.pem\n",
        "  User davipeag\n",
        "  StrictHostKeyChecking no\n",
        "\"\"\"\n",
        "\n",
        "if os.name == 'nt':\n",
        "  base_path = \"\"\n",
        "  REPO_DIR = \".\"\n",
        "  STORE_DIR =\".\" \n",
        "  print(\"Windows\")\n",
        "else:\n",
        "  print(\"Unix-like\")\n",
        "  REPO_DIR = \"/tmp/HeartRateRegression\"\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  GIT_PATH = \"/content/drive/My\\ Drive/deeplearning_project/github.pem\"\n",
        "  DATA_DIR = os.path.join(REPO_DIR, \"repo\")\n",
        "  STORE_DIR =\"/content/drive/My Drive/deeplearning_project/\" \n",
        "  !mkdir ~/.ssh\n",
        "  !cp -u {GIT_PATH} ~/.ssh/\n",
        "  !chmod u=rw,g=,o= ~/.ssh/github.pem\n",
        "  !echo \"{ssh_config}\" > ~/.ssh/config\n",
        "  !chmod u=rw,g=,o= ~/.ssh/config\n",
        "  ! (cd /tmp && git clone git@github.com:davipeag/HeartRateRegression.git)\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "  import sys\n",
        "  sys.path.append(REPO_DIR)\n",
        "\n",
        "def git_pull():\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "\n",
        "git_pull()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=9707ca04f5e04519701cb2ef5c4e353bd2a0618595a5d9576615caadbb1b2c0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Unix-like\n",
            "Mounted at /content/drive\n",
            "Cloning into 'HeartRateRegression'...\n",
            "Warning: Permanently added 'github.com,192.30.255.112' (RSA) to the list of known hosts.\n",
            "remote: Enumerating objects: 248, done.\u001b[K\n",
            "remote: Counting objects: 100% (248/248), done.\u001b[K\n",
            "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
            "remote: Total 1268 (delta 159), reused 166 (delta 80), pack-reused 1020\u001b[K\n",
            "Receiving objects: 100% (1268/1268), 88.37 MiB | 34.17 MiB/s, done.\n",
            "Resolving deltas: 100% (827/827), done.\n",
            "Warning: Permanently added the RSA host key for IP address '192.30.255.113' to the list of known hosts.\n",
            "Already up to date.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCFiZv0xM1pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaa9f0b-ae9f-4ae1-e4f6-71e8085e7215"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "args = {\n",
        "    'epoch_num': 250,     # Number of epochs.\n",
        "    'lr': 1.0e-3,           # Learning rate.\n",
        "    'weight_decay': 10e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 0,     # Number of workers on data loader.\n",
        "    'batch_size': 128,     # Mini-batch size. 128\n",
        "    'batch_test': 248,     # size of test batch\n",
        "    'window': 15,\n",
        "    'initial_window':5,\n",
        "    'clip_norm': 6.0,     # Upper limit on gradient L2 norm ###\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "SEED = 1234\n",
        "def reset_seeds():\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reset_seeds()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7V97F8pWmvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10b2a15-e2cd-4c98-a35a-1fd5d2139f79"
      },
      "source": [
        "from data_utils import (Pamap2Handler, FormatPamap)\n",
        "\n",
        "SUBJECTS = list(range(1,9))\n",
        "\n",
        "handler = Pamap2Handler(DATA_DIR)\n",
        "formatter = FormatPamap()\n",
        "dfs_train = [formatter.transform(handler.get_protocol_subject(i)) for i in SUBJECTS]\n",
        "[len(df)//200 for df in dfs_train]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "download\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1882, 2235, 1264, 1647, 1873, 1809, 1567, 2040]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdDUK1vToJWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ff1f81-7f88-4905-a2f3-2518119fa327"
      },
      "source": [
        "git_pull()\n",
        "\n",
        "import importlib\n",
        "\n",
        "import PPG\n",
        "import preprocessing_utils\n",
        "from PPG import FullTrainer\n",
        "import RegressionHR\n",
        "\n",
        "from RegressionHR import FullTrainer\n",
        "from RegressionHR import PceLstmDefaults\n",
        "from RegressionHR import PceLstmModel\n",
        "from RegressionHR import TrainerJoint\n",
        "from RegressionHR import  UtilitiesData\n",
        "\n",
        "\n",
        "importlib.reload(PPG.AttentionDefaults)\n",
        "importlib.reload(PPG)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(PPG.Models)\n",
        "importlib.reload(PPG.NoHrPceLstmModel)\n",
        "importlib.reload(PPG.TrainerXY)\n",
        "importlib.reload(PPG.TrainerIS)\n",
        "importlib.reload(PPG.FullTrainer)\n",
        "importlib.reload(PceLstmDefaults)\n",
        "importlib.reload(preprocessing_utils)\n",
        "importlib.reload(RegressionHR)\n",
        "importlib.reload(RegressionHR.FullTrainer)\n",
        "importlib.reload(RegressionHR.PceLstmDefaults)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(preprocessing_utils)\n",
        "importlib.reload(RegressionHR.TrainerJoint)\n",
        "importlib.reload(RegressionHR.UtilitiesData)\n",
        "importlib.reload(RegressionHR.PceLstmModel)\n",
        "importlib.reload(preprocessing_utils)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'preprocessing_utils' from '/tmp/HeartRateRegression/preprocessing_utils.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL7zop0WGrHb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "def compute_ensemble(results):\n",
        "  \n",
        "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
        "  min_len_y = min([len(y) for y in ys])\n",
        "  ys = [y[:min_len_y] for y in ys]\n",
        "  for i in range(1, len(ys)-1):\n",
        "    assert np.all(ys[i] == ys[i-1])\n",
        "  ps = np.stack([v[\"predictions\"][1].reshape(-1).numpy()[:min_len_y] for v in results])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "\n",
        "  return np.mean(np.abs(a - y)), np.mean(np.abs(ps - y))\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc9MVLWZAH1q"
      },
      "source": [
        "\n",
        "fchoice = {\n",
        "  'alpha': 0.6, \n",
        "  'ts_per_samples': [50], \n",
        "  'val_sub': 4,\n",
        "  'ts_sub': 5, \n",
        "  'batch_size': 64, \n",
        "  'weight_decay': 0.0, \n",
        "  'lr': 0.001, \n",
        "  'nattrs': 40,\n",
        "  'dropout_rate': 0.15, \n",
        "  'lstm_input': 64, \n",
        "  'lstm_size': 128, \n",
        "  'ts_h_size': 16}\n",
        "\n",
        "\n",
        "from PPG import UtilitiesDataXY\n",
        "from collections import defaultdict\n",
        "nepoch = 80\n",
        "aresults = defaultdict(dict)\n",
        "for val_sub in [4,5,3,2,1,0,6,7]:\n",
        "  for ts_sub in range(8):\n",
        "    if val_sub == ts_sub:\n",
        "      continue\n",
        "    fchoice[\"ts_sub\"] = ts_sub\n",
        "    fchoice[\"val_sub\"] = val_sub\n",
        "    filename = f\"pamap_ts_{ts_sub}_val_{val_sub}_alpha_{fchoice['alpha']}_nepoch_{nepoch}.pkl\"\n",
        "    save_path = os.path.join(STORE_DIR, filename)\n",
        "    try:\n",
        "      with open(save_path , \"rb\") as f:\n",
        "        out = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "      full_trainer = RegressionHR.FullTrainer.PceLstmDiscriminatorFullTrainer(dfs_train, args[\"device\"], nepoch)\n",
        "      try:\n",
        "        out = full_trainer.train(**fchoice)\n",
        "        with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(out, f)\n",
        "\n",
        "      except RuntimeError as e:\n",
        "        if isinstance(e, KeyboardInterrupt):\n",
        "          raise e\n",
        "        else:\n",
        "          print(\"####\")\n",
        "          print(f\"Failed: {choice}\")\n",
        "          print(\"###\")\n",
        "    \n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    aresults[ts_sub][val_sub] = out\n",
        "    print(f\"{ts_sub}-TS:{compute_ensemble(list(aresults[ts_sub].values()))}\")\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifYZuGK3y6BE",
        "outputId": "194f7614-7261-4727-85d8-f740a9686f77"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "fchoice = {\n",
        "  'alpha': 0.6, \n",
        "  'ts_per_samples': [50], \n",
        "  'val_sub': 4,\n",
        "  'ts_sub': 5, \n",
        "  'batch_size': 64, \n",
        "  'weight_decay': 0.0, \n",
        "  'lr': 0.001, \n",
        "  'nattrs': 40,\n",
        "  'dropout_rate': 0.15, \n",
        "  'lstm_input': 64, \n",
        "  'lstm_size': 128, \n",
        "  'ts_h_size': 16}\n",
        "\n",
        "\n",
        "from PPG import UtilitiesDataXY\n",
        "from collections import defaultdict\n",
        "\n",
        "aresults = defaultdict(dict)\n",
        "for val_sub in range(8):\n",
        "  for ts_sub in range(8):\n",
        "    if val_sub == ts_sub:\n",
        "      continue\n",
        "    fchoice[\"ts_sub\"] = ts_sub\n",
        "    fchoice[\"val_sub\"] = val_sub\n",
        "    filename = f\"pamap_ts_{ts_sub}_val_{val_sub}_alpha_{fchoice['alpha']}-joint_val.pkl\"\n",
        "    save_path = os.path.join(STORE_DIR, filename)\n",
        "    try:\n",
        "      with open(save_path , \"rb\") as f:\n",
        "        out = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "      full_trainer = RegressionHR.FullTrainer.PceLstmDiscriminatorFullTrainerJointValidation(dfs_train, args[\"device\"], 40)\n",
        "      try:\n",
        "        out = full_trainer.train(**fchoice)\n",
        "        with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(out, f)\n",
        "\n",
        "      except RuntimeError as e:\n",
        "        if isinstance(e, KeyboardInterrupt):\n",
        "          raise e\n",
        "        else:\n",
        "          print(\"####\")\n",
        "          print(f\"Failed: {choice}\")\n",
        "          print(\"###\")\n",
        "    \n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    aresults[ts_sub][val_sub] = out\n",
        "    print(f\"{ts_sub}-TS:{compute_ensemble(list(aresults[ts_sub].values()))}\")\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 0, 'ts_sub': 1, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (12.995863, 0.5780997276306152)\n",
            "1-TS:(12.995863, 12.995863)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 0, 'ts_sub': 2, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (9.420144, 0.881915271282196)\n",
            "2-TS:(9.420144, 9.420144)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 0, 'ts_sub': 3, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (26.43846, 0.5937963724136353)\n",
            "3-TS:(26.43846, 26.43846)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 0, 'ts_sub': 4, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (15.874296, 0.5779200792312622)\n",
            "4-TS:(15.874296, 15.874296)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 0, 'ts_sub': 5, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (8.995535, 0.533210039138794)\n",
            "5-TS:(8.995535, 8.995535)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 0, 'ts_sub': 6, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (13.867516, 0.6488147377967834)\n",
            "6-TS:(13.867516, 13.867516)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 0, 'ts_sub': 7, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (25.104946, 1.073097825050354)\n",
            "7-TS:(25.104946, 25.104946)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 1, 'ts_sub': 0, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (17.823238, 0.4378398358821869)\n",
            "0-TS:(17.823238, 17.823238)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 1, 'ts_sub': 2, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (6.8721175, 0.6197303533554077)\n",
            "2-TS:(7.834359, 8.1461315)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 1, 'ts_sub': 3, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (25.758514, 0.6254651546478271)\n",
            "3-TS:(25.915133, 26.098486)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 1, 'ts_sub': 4, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (18.044876, 0.6955108046531677)\n",
            "4-TS:(16.867085, 16.959587)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 1, 'ts_sub': 5, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (9.301712, 0.6170731782913208)\n",
            "5-TS:(8.889442, 9.148623)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 1, 'ts_sub': 6, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (12.741755, 1.0695477724075317)\n",
            "6-TS:(13.124559, 13.304635)\n",
            "{'alpha': 0.6, 'ts_per_samples': [50], 'val_sub': 1, 'ts_sub': 7, 'batch_size': 64, 'weight_decay': 0.0, 'lr': 0.001, 'nattrs': 40, 'dropout_rate': 0.15, 'lstm_input': 64, 'lstm_size': 128, 'ts_h_size': 16} (27.45985, 0.7502956986427307)\n",
            "7-TS:(24.238964, 24.710361)\n",
            "about to train:\n",
            "best val epoch: 1\n",
            "[1/40]: loss_train: (19.978708, 0.5427289605140686) loss_val (18.881952, 0.5009689927101135) loss_ts (17.066856, 0.6357424259185791)\n",
            "best val epoch: 2\n",
            "[2/40]: loss_train: (17.690561, 0.5534217357635498) loss_val (16.545092, 0.5235821604728699) loss_ts (16.586805, 0.6264522075653076)\n",
            "best val epoch: 3\n",
            "[3/40]: loss_train: (15.73042, 0.5324833393096924) loss_val (15.111866, 0.5024004578590393) loss_ts (19.036028, 0.59898841381073)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC7RYiJL3nh-"
      },
      "source": [
        "import random \n",
        "# options  = {'val_sub': [4],\n",
        "#   'ts_sub': [5],\n",
        "#   'batch_size': [64],\n",
        "#   'weight_decay': [0.0001, 0.000],\n",
        "#   'lr': [0.001, 0.005, 0.0001, 0.0005],\n",
        "#   'nattrs': [40],\n",
        "#   'dropout_rate': [0, 0.15, 0.25, 0.35, 0.50],\n",
        "#   'lstm_input': [32, 64, 128],\n",
        "#   'lstm_size': [16, 32, 64],\n",
        "#   'ts_h_size': [16, 32, 64],\n",
        "#   'ts_per_samples':[[40]],\n",
        "#   'alpha': [0.5, 0.6, 0.7, 0.8]\n",
        "#   }\n",
        "\n",
        "options = {\n",
        "        'val_sub': [4],\n",
        "        'ts_sub': [5],\n",
        "  'batch_size': [64],\n",
        "  'weight_decay': [0.0001, 0.00001, 0.0],\n",
        "  'lr': [0.001, 0.005, 0.0001, 0.0005],\n",
        "  'nattrs': [40],\n",
        "  'dropout_rate': [0, 0.15,0.25, 0.5],\n",
        "  'lstm_input': [16, 32, 64, 128],\n",
        "  'lstm_size': [16, 32, 64, 128],\n",
        "  'ts_h_size': [16, 32, 64, 128],\n",
        "  'ts_per_samples':[[30], [40], [50]],\n",
        "  'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "  }\n",
        "\n",
        "\n",
        "def choose(options):\n",
        "  choice = dict()\n",
        "  for k,v in options.items():\n",
        "    choice[k] = random.choice(v)\n",
        "  return choice\n",
        "\n",
        "from  Optimization.Optimizers import RandomSearch\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "filename = \"pamap_pce_lstm_discriminator_results.pkl\"\n",
        "save_path = os.path.join(STORE_DIR, filename)\n",
        "\n",
        "try:\n",
        "  with open(save_path, \"rb\") as f:\n",
        "    results = pickle.load(f)\n",
        "except FileNotFoundError:\n",
        "  results = list()\n",
        "\n",
        "full_trainer = RegressionHR.FullTrainer.PceLstmDiscriminatorFullTrainer(dfs_train, args[\"device\"], 40)\n",
        "searcher = RandomSearch(full_trainer, options)\n",
        "searcher.results = results\n",
        "while True:\n",
        "  searcher.fit(1)\n",
        "  with open(save_path, \"wb\") as f:\n",
        "    pickle.dump(searcher.results, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjYv8BlUKYnM"
      },
      "source": [
        "\n",
        "def compute_ensemble(results):\n",
        "  ps = [v[\"predictions\"][1].reshape(-1).numpy() for v in results]\n",
        "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
        "\n",
        "  for i in range(1, len(ys)-1):\n",
        "    assert np.all(ys[i] == ys[i-1])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "\n",
        "  np.mean(np.abs(a - y))\n",
        "\n",
        "compute_ensemble(dresults)\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# p = p.reshape(-1)\n",
        "\n",
        "# plt.plot(y)\n",
        "# plt.plot(p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdPECC3jYpjp"
      },
      "source": [
        "import random \n",
        "options = {\n",
        "  \"ts_h_size\": [64, 128],\n",
        "  \"lstm_size\": [64, 64, 128],\n",
        "  \"lstm_input\": [64, 128, 256],\n",
        "  \"dropout_rate\": [0.25],\n",
        "  \"bvp_count\": [8,16],\n",
        "  \"nattrs\": [5],\n",
        "  'lr': [0.001],\n",
        "  'weight_decay': [0, 0.0001],\n",
        "  'batch_size': [64, 128, 256],\n",
        "  'ts_sub': [0],\n",
        "  'val_sub': [4]\n",
        " }\n",
        "\n",
        "def choose(options):\n",
        "  choice = dict()\n",
        "  for k,v in options.items():\n",
        "    choice[k] = random.choice(v)\n",
        "  return choice\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYFUSi1-4nzj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YFiismva8BL"
      },
      "source": [
        "results = list()\n",
        "from PPG import UtilitiesDataXY\n",
        "while True:\n",
        "  full_trainer = FullTrainer.NoHrPceLstmFullTrainer(dfs_train, args[\"device\"])\n",
        "  choice = choose(options)\n",
        "  try:\n",
        "    out = full_trainer.train(**choice)\n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    results.append([out[\"args\"], out[\"metric\"]])\n",
        "  except RuntimeError as e:\n",
        "    if isinstance(e, KeyboardInterrupt):\n",
        "      raise e\n",
        "    else:\n",
        "      print(\"####\")\n",
        "      print(f\"Failed: {choice}\")\n",
        "      print(\"###\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNTrC3_M2D-Z"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chhDawFV_14T"
      },
      "source": [
        "fchoice = {'val_sub': 4,\n",
        "  'ts_sub': 0,\n",
        "  'batch_size': 64,\n",
        "  'weight_decay': 0,\n",
        "  'lr': 0.001,\n",
        "  'nattrs': 5,\n",
        "  'bvp_count': 16,\n",
        "  'dropout_rate': 0.25,\n",
        "  'lstm_input': 128,\n",
        "  'lstm_size': 64,\n",
        "  'ts_h_size': 64}\n",
        "\n",
        "\n",
        "dresults = list()\n",
        "from PPG import UtilitiesDataXY\n",
        "ts_sub = 3\n",
        "for val_sub in [i for i in range(15) if i != ts_sub]:\n",
        "  full_trainer = FullTrainer.NoHrPceLstmFullTrainer(dfs_train, args[\"device\"])\n",
        "  try:\n",
        "    fchoice[\"ts_sub\"] = ts_sub\n",
        "    fchoice[\"val_sub\"] = val_sub\n",
        "    out = full_trainer.train(**fchoice)\n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    dresults.append([out])\n",
        "  except RuntimeError as e:\n",
        "    if isinstance(e, KeyboardInterrupt):\n",
        "      raise e\n",
        "    else:\n",
        "      print(\"####\")\n",
        "      print(f\"Failed: {choice}\")\n",
        "      print(\"###\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjMx66OD9T9"
      },
      "source": [
        "y = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFOG4ar9WYpt"
      },
      "source": [
        "full_trainer = FullTrainer.AttentionFullTrainer(dfs_train, args[\"device\"], 0, 1)\n",
        "\n",
        "full_trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYVLTnMGXYGp"
      },
      "source": [
        "from PPG import UtilitiesDataXY \n",
        "\n",
        "\n",
        "transformers = PPG.AttentionDefaults.get_preprocessing_transformer()\n",
        "make_loaders = UtilitiesDataXY.DataLoaderFactory(transformers, dfs_train).make_loaders\n",
        "\n",
        "loader_tr, loader_val, loader_ts = make_loaders(ts_sub=0, val_sub=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Vr-YDyoGfH"
      },
      "source": [
        "from PPG.Models import SnippetConvolutionalTransformer\n",
        "\n",
        "net = SnippetConvolutionalTransformer().to(args[\"device\"])\n",
        "\n",
        "# x,y = next(iter(loader_tr))\n",
        "\n",
        "# p = net(x)\n",
        "\n",
        "criterion = nn.MSELoss().to(args[\"device\"])# nn.L1Loss().to(args[\"device\"]) #nn.CrossEntropyLoss().to(args[\"device\"])\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=args[\"lr\"],\n",
        "                             weight_decay=args[\"weight_decay\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ktvTpTaOcy"
      },
      "source": [
        "from PPG.TrainerXY import (EpochTrainerXY, MetricsComputerXY, TrainHelperXY)\n",
        "from preprocessing_utils import ZTransformer2\n",
        "\n",
        "epoch_trainer = EpochTrainerXY(net, optimizer, criterion, args[\"device\"])\n",
        "ztransformer = ZTransformer2(['heart_rate', 'wrist-ACC-0', 'wrist-ACC-1', 'wrist-ACC-2',\n",
        "              'wrist-BVP-0', 'wrist-EDA-0', 'wrist-TEMP-0', 'chest-ACC-0',\n",
        "              'chest-ACC-1', 'chest-ACC-2', 'chest-Resp-0'])\n",
        "metrics_comuter = MetricsComputerXY(ztransformer)\n",
        "\n",
        "train_helper = TrainHelperXY(epoch_trainer, loader_tr, loader_val, loader_ts, metrics_comuter.mae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lyVMlfzkaFA"
      },
      "source": [
        "train_helper.train(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0tSXLWPuCQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}