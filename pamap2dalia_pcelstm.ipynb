{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "pamap2dalia_pcelstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFeV0gY5n8La",
        "outputId": "1897051a-add0-408c-bf32-edc94106fe1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "ssh_config = \"\"\"\n",
        "Host github.com\n",
        "  IdentityFile ~/.ssh/github.pem\n",
        "  User davipeag\n",
        "  StrictHostKeyChecking no\n",
        "\"\"\"\n",
        "\n",
        "if os.name == 'nt':\n",
        "  base_path = \"\"\n",
        "  REPO_DIR = \".\"\n",
        "  STORE_DIR =\".\" \n",
        "  print(\"Windows\")\n",
        "else:\n",
        "  print(\"Unix-like\")\n",
        "  REPO_DIR = \"/tmp/HeartRateRegression\"\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  GIT_PATH = \"/content/drive/My\\ Drive/deeplearning_project/github.pem\"\n",
        "  DATA_DIR = os.path.join(REPO_DIR, \"repo\")\n",
        "  STORE_DIR =\"/content/drive/My Drive/deeplearning_project/\" \n",
        "  !mkdir ~/.ssh\n",
        "  !cp -u {GIT_PATH} ~/.ssh/\n",
        "  !chmod u=rw,g=,o= ~/.ssh/github.pem\n",
        "  !echo \"{ssh_config}\" > ~/.ssh/config\n",
        "  !chmod u=rw,g=,o= ~/.ssh/config\n",
        "  ! (cd /tmp && git clone git@github.com:davipeag/HeartRateRegression.git)\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "  import sys\n",
        "  sys.path.append(REPO_DIR)\n",
        "\n",
        "def git_pull():\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "\n",
        "git_pull()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=b52a718d58259385b92f4646256f97741d72ca5afb3625d8d6782e79826f8ba7\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Unix-like\n",
            "Mounted at /content/drive\n",
            "Cloning into 'HeartRateRegression'...\n",
            "Warning: Permanently added 'github.com,140.82.121.4' (RSA) to the list of known hosts.\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 1803 (delta 44), reused 45 (delta 20), pack-reused 1722\u001b[K\n",
            "Receiving objects: 100% (1803/1803), 128.69 MiB | 9.80 MiB/s, done.\n",
            "Resolving deltas: 100% (1189/1189), done.\n",
            "Warning: Permanently added the RSA host key for IP address '140.82.121.3' to the list of known hosts.\n",
            "Already up to date.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEbepTtKn8Lw",
        "outputId": "75b444b0-eb74-48ae-feef-35994940aace",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "args = {\n",
        "    'epoch_num': 250,     # Number of epochs.\n",
        "    'lr': 1.0e-3,           # Learning rate.\n",
        "    'weight_decay': 10e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 0,     # Number of workers on data loader.\n",
        "    'batch_size': 128,     # Mini-batch size. 128\n",
        "    'batch_test': 248,     # size of test batch\n",
        "    'window': 15,\n",
        "    'initial_window':5,\n",
        "    'clip_norm': 6.0,     # Upper limit on gradient L2 norm ###\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "SEED = 1234\n",
        "def reset_seeds(seed=SEED):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reset_seeds(SEED)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ksv2MPn8Ly",
        "outputId": "aa702454-461d-4851-986c-7b205b5311b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from data_utils import (PpgDaliaExtractor, FormatPPGDalia)\n",
        "\n",
        "extractor = PpgDaliaExtractor(DATA_DIR)\n",
        "ppg_dalia_formatter = FormatPPGDalia()\n",
        "dfs_train_dalia = [ppg_dalia_formatter.transform(extractor.extract_subject(i)) for i in range(1,16)]\n",
        "[len(df)//32 for df in dfs_train_dalia]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9206,\n",
              " 8199,\n",
              " 8735,\n",
              " 9144,\n",
              " 9299,\n",
              " 5244,\n",
              " 9337,\n",
              " 8074,\n",
              " 8554,\n",
              " 10642,\n",
              " 9042,\n",
              " 7908,\n",
              " 9130,\n",
              " 8952,\n",
              " 7933]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O1eIe8On8Ly",
        "outputId": "1250a1c9-595a-42a9-c6a6-bdfe48f951d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from data_utils import (Pamap2Handler, FormatPamap)\n",
        "\n",
        "SUBJECTS = list(range(1,9))\n",
        "\n",
        "handler = Pamap2Handler(DATA_DIR)\n",
        "formatter = FormatPamap()\n",
        "dfs_train_pamap2 = [formatter.transform(handler.get_protocol_subject(i)) for i in SUBJECTS]\n",
        "[len(df)//200 for df in dfs_train_pamap2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "download\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1882, 2235, 1264, 1647, 1873, 1809, 1567, 2040]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0YeDWNdn8Lz",
        "outputId": "6fc565b3-1f30-45ec-ec02-5c8750ab58df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "git_pull()\n",
        "\n",
        "import importlib\n",
        "\n",
        "import PPG\n",
        "import preprocessing_utils\n",
        "from PPG import FullTrainer\n",
        "import RegressionHR\n",
        "\n",
        "from RegressionHR import FullTrainer\n",
        "from RegressionHR import PceLstmDefaults\n",
        "from RegressionHR import PceLstmModel\n",
        "from RegressionHR import TrainerJoint\n",
        "from RegressionHR import  UtilitiesData\n",
        "from RegressionHR import FullTrainer2\n",
        "\n",
        "import Trainer\n",
        "from  Trainer import BatchTrainers\n",
        "from Trainer import BatchComputers\n",
        "from Trainer import Interfaces\n",
        "\n",
        "\n",
        "importlib.reload(PPG.AttentionDefaults)\n",
        "importlib.reload(PPG)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(PPG.Models)\n",
        "importlib.reload(PPG.NoHrPceLstmModel)\n",
        "importlib.reload(PPG.TrainerXY)\n",
        "importlib.reload(PPG.TrainerIS)\n",
        "importlib.reload(PPG.FullTrainer)\n",
        "importlib.reload(PceLstmDefaults)\n",
        "importlib.reload(preprocessing_utils)\n",
        "importlib.reload(RegressionHR)\n",
        "importlib.reload(RegressionHR.FullTrainer)\n",
        "importlib.reload(RegressionHR.FullTrainer2)\n",
        "importlib.reload(RegressionHR.PceLstmDefaults)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(preprocessing_utils)\n",
        "importlib.reload(RegressionHR.TrainerJoint)\n",
        "importlib.reload(RegressionHR.UtilitiesData)\n",
        "importlib.reload(RegressionHR.PceLstmModel)\n",
        "\n",
        "importlib.reload(preprocessing_utils)\n",
        "importlib.reload(Trainer)\n",
        "importlib.reload(Trainer.BatchTrainers)\n",
        "importlib.reload(Trainer.BatchComputers)\n",
        "importlib.reload(Trainer.ToolBox)\n",
        "importlib.reload(Trainer.Interfaces )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'Trainer.Interfaces' from '/tmp/HeartRateRegression/Trainer/Interfaces.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJd9nxv_n8L0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "def compute_ensemble(results):\n",
        "  \n",
        "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
        "  min_len_y = min([len(y) for y in ys])\n",
        "  ys = [y[:min_len_y] for y in ys]\n",
        "  for i in range(1, len(ys)-1):\n",
        "    # assert np.all(ys[i] == ys[i-1])\n",
        "    assert np.all(np.abs(ys[i] - ys[i-1])<1)\n",
        "  ps = np.stack([v[\"predictions\"][1].reshape(-1).numpy()[:min_len_y] for v in results])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "  plt.show()\n",
        "\n",
        "  return np.mean(np.abs(a - y)), np.mean(np.abs(ps - y))\n",
        "\n",
        "\n",
        "def compute_ensemble_named(results, model_name=\"PceLstm\"):\n",
        "  \n",
        "  results = [r[model_name] for r in results]\n",
        "\n",
        "  ys = [v[\"labels\"].reshape(-1) for v in results]\n",
        "  min_len_y = min([len(y) for y in ys])\n",
        "  ys = [y[:min_len_y] for y in ys]\n",
        "  for i in range(1, len(ys)-1):\n",
        "    # assert np.all(ys[i] == ys[i-1])\n",
        "    assert np.all(np.abs(ys[i] - ys[i-1])<1)\n",
        "  ps = np.stack([v[\"predictions\"].reshape(-1)[:min_len_y] for v in results])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "  plt.show()\n",
        "\n",
        "  return np.mean(np.abs(a - y)), np.mean(np.abs(ps - y))\n",
        "\n",
        "# compute_ensemble_named(aresults[0].values())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKNHPOI1n8L2",
        "outputId": "1063bffd-ae5c-4d54-b8b4-56fb1677d5f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fchoice = {\n",
        "    'is_h_size': 64,\n",
        "    'ts_per_is': 12,\n",
        "    'period_s': 4,\n",
        "    'step_s': 2,\n",
        "    'alpha': 0.90,\n",
        "    'ts_per_sample': 50,\n",
        "    'batch_size': 64,\n",
        "    'weight_decay': 1e-05,\n",
        "    'lr': 0.005,\n",
        "    'dropout_rate': 0.15,\n",
        "    'lstm_input': 128,\n",
        "    'lstm_size': 64,\n",
        "    'ts_h_size': 16,\n",
        "    'alpha': 0.9,\n",
        "    'margin': 1,\n",
        "}\n",
        "\n",
        "discriminator_false_label = 0\n",
        "from RegressionHR import FullTrainer2\n",
        "import Trainer.BatchTrainers\n",
        "from PPG import UtilitiesDataXY\n",
        "from collections import defaultdict\n",
        "import torch \n",
        "\n",
        "nepoch = 100\n",
        "aresults_pamap2 = defaultdict(dict)\n",
        "aresults_dalia = defaultdict(dict)\n",
        "pamap2_size = 8\n",
        "dalia_size = 15\n",
        "for val_sub in range(min(pamap2_size, dalia_size)):\n",
        "  for ts_sub1 in range(pamap2_size):\n",
        "    if val_sub == ts_sub1:\n",
        "      continue\n",
        "    val_sub1 = val_sub\n",
        "    val_sub2 = val_sub\n",
        "    ts_sub2 = ts_sub1\n",
        "\n",
        "    filename = f\"pamap_ts_{ts_sub1}_val_{val_sub1}_dalia_ts_{ts_sub2}_val_{val_sub2}_alpha_{fchoice['alpha']}_lr_{fchoice['lr']}-joint_val_nepoch_{nepoch}_triplet.pkl\"\n",
        "    save_path = os.path.join(STORE_DIR, filename)\n",
        "    try:\n",
        "      with open(save_path , \"rb\") as f:\n",
        "        out = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "      full_trainer = FullTrainer2.PceLstmTripletDiscriminatorPamap2DaliaJointTraining(\n",
        "        dfs_ds1 = dfs_train_pamap2,\n",
        "        dfs_ds2 = dfs_train_dalia,\n",
        "        device = args[\"device\"],\n",
        "        nepoch = nepoch,\n",
        "        ts_sub1 = ts_sub1,\n",
        "        ts_sub2 = ts_sub2,\n",
        "        val_sub1 = val_sub1,\n",
        "        val_sub2 = val_sub2,\n",
        "        main_index = 0\n",
        "        )\n",
        "      \n",
        "      out = full_trainer.train(**fchoice)\n",
        "      with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(out, f)    \n",
        "    print(out[\"args\"], [out[k]['metric'] for k in (full_trainer.lstm_name1,  full_trainer.lstm_name2, full_trainer.disciminator_name1, full_trainer.disciminator_name2)])\n",
        "    aresults_pamap2[ts_sub1][val_sub1] = out\n",
        "    print(f\"{ts_sub1}-TS:{compute_ensemble_named(list(aresults_pamap2[ts_sub1].values()), full_trainer.lstm_name2)}\")\n",
        "  for ts_sub2 in range(dalia_size):\n",
        "    if val_sub == ts_sub2:\n",
        "      continue\n",
        "    val_sub1 = val_sub\n",
        "    val_sub2 = val_sub\n",
        "    ts_sub1 = ts_sub2 % pamap2_size\n",
        "    filename = f\"dalia_ts_{ts_sub2}_val_{val_sub2}_pamap2_ts_{ts_sub1}_val_{val_sub1}_alpha_{fchoice['alpha']}_lr_{fchoice['lr']}-joint_val_nepoch_{nepoch}_triplet.pkl\"\n",
        "    save_path = os.path.join(STORE_DIR, filename)\n",
        "    try:\n",
        "      with open(save_path , \"rb\") as f:\n",
        "        out = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "      full_trainer = FullTrainer2.PceLstmTripletDiscriminatorPamap2DaliaJointTraining(\n",
        "        dfs_ds1 = dfs_train_pamap2,\n",
        "        dfs_ds2 = dfs_train_dalia,\n",
        "        device = args[\"device\"],\n",
        "        nepoch = nepoch,\n",
        "        ts_sub1 = ts_sub1,\n",
        "        ts_sub2 = ts_sub2,\n",
        "        val_sub1 = val_sub1,\n",
        "        val_sub2 = val_sub2,\n",
        "        main_index = 1\n",
        "        )\n",
        "      out = full_trainer.train(**fchoice)\n",
        "      with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(out, f)\n",
        "    print(out[\"args\"], [out[k]['metric'] for k in (full_trainer.lstm_name1,  full_trainer.lstm_name2, full_trainer.disciminator_name1, full_trainer.disciminator_name2)])\n",
        "    aresults_dalia[ts_sub2][val_sub2] = out\n",
        "    print(f\"{ts_sub2}-TS:{compute_ensemble_named(list(aresults_dalia[ts_sub1].values()), full_trainer.lstm_name2)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ts_per_is: 12\n",
            "sample_per_ts: 400\n",
            "ts_per_is: 12\n",
            "sample_per_ts: 128\n",
            "best val epoch: 1\n",
            "[1/100]: loss_train: [20.739332, 18.37647, 0.9984704318260889, 1.0036534539039066] loss_val [18.624647, 13.765243, 0.9722293801089733, 1.0000000001241025] loss_ts [22.344332, 18.485308, 0.9999999998348902, 0.9773606751856528]\n",
            "best val epoch: 2\n",
            "[2/100]: loss_train: [20.367867, 15.997773, 0.9989666351099947, 0.9872147343541857] loss_val [18.497616, 14.889048, 0.9583951550590034, 1.0002487411568521] loss_ts [21.957983, 17.788136, 1.0000000018574855, 1.0060467780310083]\n",
            "best val epoch: 3\n",
            "[3/100]: loss_train: [18.758627, 16.221977, 0.9981056954941632, 0.9855331510108145] loss_val [17.793236, 10.037525, 0.954131672050484, 1.0002880905849603] loss_ts [20.361998, 15.672395, 0.9999999992570058, 0.9921350446504755]\n",
            "best val epoch: 5\n",
            "[5/100]: loss_train: [14.260524, 14.371442, 0.9903202217135486, 0.9661666942184739] loss_val [13.528313, 11.333172, 1.0015133610227431, 1.000277829039033] loss_ts [16.207401, 13.990603, 0.9999999980186822, 0.9829192955195372]\n",
            "best val epoch: 7\n",
            "[7/100]: loss_train: [12.297679, 11.194042, 0.9998459294179661, 1.0015760656145478] loss_val [12.728453, 7.4963317, 0.9432888960986917, 1.000427708836413] loss_ts [13.715784, 10.920358, 0.9999999990093411, 0.9996262223574465]\n",
            "best val epoch: 10\n",
            "[10/100]: loss_train: [12.253067, 10.411945, 1.001442317687044, 0.9826460605836473] loss_val [10.569546, 11.168696, 0.953317337997072, 1.0004067965592038] loss_ts [14.298303, 10.370814, 1.000572408913245, 1.0363413187221429]\n",
            "best val epoch: 13\n",
            "[13/100]: loss_train: [8.862604, 9.378188, 0.9897505864069832, 1.0210659844871972] loss_val [10.2630205, 7.039462, 1.040405178334244, 1.0003019738488081] loss_ts [10.576356, 9.599224, 0.9999999984314567, 0.9776461094103704]\n",
            "best val epoch: 16\n",
            "[16/100]: loss_train: [8.449165, 9.515827, 0.9945036727233709, 1.0064993859592102] loss_val [9.644045, 24.541973, 0.988594759121496, 1.0005710215564287] loss_ts [9.762203, 8.743884, 1.0000721472122003, 0.975160793498256]\n",
            "best val epoch: 19\n",
            "[19/100]: loss_train: [7.4493685, 7.5422716, 0.9945218096958635, 1.0008206738291645] loss_val [9.353588, 31.260546, 1.0136335792633966, 1.000491563405826] loss_ts [8.737957, 8.376156, 1.000136173663047, 0.971626916034884]\n",
            "best val epoch: 24\n",
            "[24/100]: loss_train: [6.2765064, 7.540035, 0.9971376055805588, 0.9879332383069495] loss_val [8.254007, 7.2235103, 0.9852215040918862, 1.000621791248659] loss_ts [7.9082966, 6.739341, 1.000327352754297, 0.9803786711597783]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhQdezTYn8L8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUjQlXk6n8MA"
      },
      "source": [
        "def compute_metric_mean(values, k):\n",
        "  return np.mean([v[k]['metric'] for v in values])\n",
        "\n",
        "ms = dict()\n",
        "\n",
        "for k,values in aresults.items():\n",
        "  vals = list(aresults[k].values())\n",
        "  e, m = compute_ensemble_named(vals)\n",
        "  t = compute_metric_mean(vals, 'PceDiscriminator')\n",
        "  print(f\"{k}: {e} {m} {t}\")\n",
        "  ms[k] = (e, m, t)\n",
        "\n",
        "print()\n",
        "for k,v in ms.items():\n",
        "  print(k, v)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}