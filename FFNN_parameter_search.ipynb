{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "ssh_config = \"\"\"\n",
    "Host github.com\n",
    "  IdentityFile ~/.ssh/github.pem\n",
    "  User davipeag\n",
    "  StrictHostKeyChecking no\n",
    "\"\"\"\n",
    "\n",
    "if os.name == 'nt':\n",
    "  base_path = \"\"\n",
    "  REPO_DIR = \".\"\n",
    "  STORE_DIR =\".\" \n",
    "  print(\"Windows\")\n",
    "else:\n",
    "  print(\"Unix-like\")\n",
    "  REPO_DIR = \"/tmp/HeartRateRegression\"\n",
    "  from google.colab import drive, auth\n",
    "  drive.mount('/content/drive')\n",
    "  GIT_PATH = \"/content/drive/My\\ Drive/deeplearning_project/github.pem\"\n",
    "  DATA_DIR = os.path.join(REPO_DIR, \"repo\")\n",
    "  STORE_DIR =\"/content/drive/My Drive/deeplearning_project/\" \n",
    "  !mkdir ~/.ssh\n",
    "  !cp -u {GIT_PATH} ~/.ssh/\n",
    "  !chmod u=rw,g=,o= ~/.ssh/github.pem\n",
    "  !echo \"{ssh_config}\" > ~/.ssh/config\n",
    "  !chmod u=rw,g=,o= ~/.ssh/config\n",
    "  ! (cd /tmp && git clone git@github.com:davipeag/HeartRateRegression.git)\n",
    "  ! (cd {REPO_DIR} && git pull )\n",
    "  import sys\n",
    "  sys.path.append(REPO_DIR)\n",
    "\n",
    "def git_pull():\n",
    "  ! (cd {REPO_DIR} && git pull )\n",
    "\n",
    "git_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "args = dict()\n",
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "print(args['device'])\n",
    "\n",
    "SEED = 1234\n",
    "def reset_seeds():\n",
    "  random.seed(SEED)\n",
    "  np.random.seed(SEED)\n",
    "  torch.manual_seed(SEED)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.cuda.manual_seed(SEED)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# reset_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import (Pamap2Handler, FormatPamap)\n",
    "\n",
    "SUBJECTS = list(range(1,9))\n",
    "\n",
    "handler = Pamap2Handler(DATA_DIR)\n",
    "formatter = FormatPamap()\n",
    "dfs_train = [formatter.transform(handler.get_protocol_subject(i)) for i in SUBJECTS]\n",
    "[len(df)//200 for df in dfs_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_pull()\n",
    "\n",
    "import importlib\n",
    "\n",
    "import PPG\n",
    "import preprocessing_utils\n",
    "from PPG import FullTrainer\n",
    "\n",
    "import RegressionHR\n",
    "importlib.reload(RegressionHR)\n",
    "\n",
    "from RegressionHR import FullTrainer\n",
    "from RegressionHR import PceLstmDefaults\n",
    "from RegressionHR import PceLstmModel\n",
    "from RegressionHR import TrainerJoint\n",
    "from RegressionHR import  UtilitiesData\n",
    "from RegressionHR import *\n",
    "from RegressionHR import Preprocessing\n",
    "import Models\n",
    "\n",
    "importlib.reload(Models.BaseModels)\n",
    "\n",
    "# importlib.reload(PPG.AttentionDefaults)\n",
    "importlib.reload(PPG)\n",
    "# importlib.reload(PPG.UtilitiesDataXY)\n",
    "importlib.reload(PPG.Models)\n",
    "# importlib.reload(PPG.NoHrPceLstmModel)\n",
    "# importlib.reload(PPG.TrainerXY)\n",
    "# importlib.reload(PPG.TrainerIS)\n",
    "# importlib.reload(PPG.FullTrainer)\n",
    "# importlib.reload(PceLstmDefaults)\n",
    "# importlib.reload(preprocessing_utils)\n",
    "\n",
    "importlib.reload(RegressionHR.FullTrainer)\n",
    "importlib.reload(RegressionHR.PceLstmDefaults)\n",
    "importlib.reload(PPG.UtilitiesDataXY)\n",
    "importlib.reload(preprocessing_utils)\n",
    "importlib.reload(RegressionHR.TrainerJoint)\n",
    "importlib.reload(RegressionHR.UtilitiesData)\n",
    "importlib.reload(RegressionHR.PceLstmModel)\n",
    "importlib.reload(preprocessing_utils)\n",
    "# import imp\n",
    "# for module in sys.modules.values():\n",
    "#     importlib.reload(module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "options = {\n",
    "  'ts_sub': [5],\n",
    "  'batch_size': [64],\n",
    "  'weight_decay': [0.0001, 0.00001, 0.0],\n",
    "  'lr': [0.001, 0.005, 0.0001, 0.0005],\n",
    "  'ts_per_sample':[30, 40, 50],\n",
    "  \"layer_sizes\": [(4,4,4), (8,8,8), (16, 16, 16), (32,32,32), (64,64,64)],\n",
    "  \"skip_mapping\": [((0,2),(0,3),(1,3)), ((0,2),(0,3)), ((1,3),(0,3)),  ((0,3),)],\n",
    "  \"dropout_rate\": [0.1]\n",
    "  }\n",
    "\n",
    "\n",
    "from  Optimization.Optimizers import RandomSearch\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "filename = \"pamap_ffnn_discriminator_results.pkl\"\n",
    "save_path = os.path.join(STORE_DIR, filename)\n",
    "\n",
    "try:\n",
    "  with open(save_path, \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "  results = list()\n",
    "\n",
    "full_trainer = RegressionHR.FullTrainer.IteractiveFFNNFullTrainerJointValidation(dfs_train, args[\"device\"], nepoch =  500)\n",
    "\n",
    "searcher = RandomSearch(full_trainer, options)\n",
    "searcher.results = results\n",
    "while True:\n",
    "  searcher.fit(1)\n",
    "  with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(searcher.results, f)"
   ]
  }
 ]
}