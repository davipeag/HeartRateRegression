{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "ppg.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksg4pPcCWcJR",
        "outputId": "3de0c1f5-6f9e-4bb1-895d-5fe0ae54ca6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "ssh_config = \"\"\"\n",
        "Host github.com\n",
        "  IdentityFile ~/.ssh/github.pem\n",
        "  User davipeag\n",
        "  StrictHostKeyChecking no\n",
        "\"\"\"\n",
        "\n",
        "if os.name == 'nt':\n",
        "  base_path = \"\"\n",
        "  REPO_DIR = \".\"\n",
        "  STORE_DIR =\".\" \n",
        "  print(\"Windows\")\n",
        "else:\n",
        "  print(\"Unix-like\")\n",
        "  REPO_DIR = \"/tmp/HeartRateRegression\"\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  GIT_PATH = \"/content/drive/My\\ Drive/deeplearning_project/github.pem\"\n",
        "  DATA_DIR = os.path.join(REPO_DIR, \"repo\")\n",
        "  STORE_DIR =\"/content/drive/My Drive/deeplearning_project/\" \n",
        "  !mkdir ~/.ssh\n",
        "  !cp -u {GIT_PATH} ~/.ssh/\n",
        "  !chmod u=rw,g=,o= ~/.ssh/github.pem\n",
        "  !echo \"{ssh_config}\" > ~/.ssh/config\n",
        "  !chmod u=rw,g=,o= ~/.ssh/config\n",
        "  ! (cd /tmp && git clone git@github.com:davipeag/HeartRateRegression.git)\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "  import sys\n",
        "  sys.path.append(REPO_DIR)\n",
        "\n",
        "def git_pull():\n",
        "  ! (cd {REPO_DIR} && git pull )\n",
        "\n",
        "git_pull()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=ef4480e26ee6625710062994831d0e1c22091c944c3fccc82b0454803aa2190b\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Unix-like\n",
            "Mounted at /content/drive\n",
            "Cloning into 'HeartRateRegression'...\n",
            "Warning: Permanently added 'github.com,192.30.255.113' (RSA) to the list of known hosts.\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 802 (delta 46), reused 36 (delta 15), pack-reused 719\u001b[K\n",
            "Receiving objects: 100% (802/802), 88.08 MiB | 30.32 MiB/s, done.\n",
            "Resolving deltas: 100% (502/502), done.\n",
            "Already up to date.\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCFiZv0xM1pa",
        "outputId": "aa2a9855-ca4f-4cc7-f625-9de0cccdac97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "args = {\n",
        "    'epoch_num': 250,     # Number of epochs.\n",
        "    'lr': 1.0e-3,           # Learning rate.\n",
        "    'weight_decay': 10e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 0,     # Number of workers on data loader.\n",
        "    'batch_size': 128,     # Mini-batch size. 128\n",
        "    'batch_test': 248,     # size of test batch\n",
        "    'window': 15,\n",
        "    'initial_window':5,\n",
        "    'clip_norm': 6.0,     # Upper limit on gradient L2 norm ###\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "SEED = 1234\n",
        "def reset_seeds():\n",
        "  random.seed(SEED)\n",
        "  np.random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reset_seeds()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7V97F8pWmvK",
        "outputId": "2ac46c2d-904c-4944-bee2-a6e1eb1d913e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from data_utils import (FormatIeee,  IeeeExtractor)\n",
        "\n",
        "SUBJECTS = list(range(1,13))\n",
        "\n",
        "extractor = IeeeExtractor(DATA_DIR)\n",
        "formatter = FormatIeee()\n",
        "dfs_train = [formatter.transform(extractor.extract_subject(i)) for i in SUBJECTS]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdDUK1vToJWo",
        "outputId": "47ff73b7-9496-4f61-caa8-09e1e0649142",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "git_pull()\n",
        "\n",
        "import importlib\n",
        "\n",
        "import PPG\n",
        "\n",
        "from PPG import FullTrainer\n",
        "\n",
        "importlib.reload(PPG.AttentionDefaults)\n",
        "importlib.reload(PPG)\n",
        "importlib.reload(PPG.UtilitiesDataXY)\n",
        "importlib.reload(PPG.Models)\n",
        "importlib.reload(PPG.NoHrPceLstmModel)\n",
        "importlib.reload(PPG.TrainerXY)\n",
        "importlib.reload(PPG.TrainerIS)\n",
        "importlib.reload(PPG.FullTrainer)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Permanently added the RSA host key for IP address '192.30.255.112' to the list of known hosts.\r\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'PPG.FullTrainer' from '/tmp/HeartRateRegression/PPG/FullTrainer.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL7zop0WGrHb",
        "outputId": "ebee82df-dffa-4840-c0c6-c9d1e95c578e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "# fchoice = {'val_sub': 4,\n",
        "#   'ts_sub': 0,\n",
        "#   'batch_size': 64,\n",
        "#   'weight_decay': 0,\n",
        "#   'lr': 0.001,\n",
        "#   'nattrs': 5,\n",
        "#   'bvp_count': 16,\n",
        "#   'dropout_rate': 0.25,\n",
        "#   'lstm_input': 128,\n",
        "#   'lstm_size': 64,\n",
        "#   'ts_h_size': 64\n",
        "#   }\n",
        "def compute_ensemble(results):\n",
        "  ps = [v[\"predictions\"][1].reshape(-1).numpy() for v in results]\n",
        "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
        "\n",
        "  for i in range(1, len(ys)-1):\n",
        "    assert np.all(ys[i] == ys[i-1])\n",
        "\n",
        "  s = ps[0]\n",
        "  for p in ps[1:]:\n",
        "    s = s + p\n",
        "\n",
        "  a = s/len(ps)\n",
        "  y = ys[0]\n",
        "\n",
        "  plt.plot(a)\n",
        "  plt.plot(y)\n",
        "\n",
        "  return np.mean(np.abs(a - y))\n",
        "\n",
        "\n",
        "fchoice = {'val_sub': 4,\n",
        "  'ts_sub': 0,\n",
        "  'batch_size': 64,\n",
        "  'weight_decay': 0,\n",
        "  'lr': 0.0001,\n",
        "  'lin_dropout': 0,\n",
        "  'lin_size': 16,\n",
        "  'nlin_layers': 2,\n",
        "  'feedforward_expansion': 1,\n",
        "  'nhead': 4,\n",
        "  'ndec_layers': 2,\n",
        "  'nenc_layers': 2,\n",
        "  'conv_dropout': 0,\n",
        "  'nconv_layers': 2,\n",
        "  'conv_filters': 128,\n",
        "  'nfeatures': 5\n",
        "}\n",
        "\n",
        "\n",
        "from PPG import UtilitiesDataXY\n",
        "\n",
        "\n",
        "aresults = list()\n",
        "for ts_sub in [0,1,2,3, 4,5,6,7,8,9,10,11]:\n",
        "  dresults = list()\n",
        "  for i in range(7):\n",
        "    filename = f\"ieee_train_ts_attention_{ts_sub}_{i}.pkl\"\n",
        "    save_path = os.path.join(STORE_DIR, filename)\n",
        "    try:\n",
        "      with open(save_path , \"rb\") as f:\n",
        "        out = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "      full_trainer = FullTrainer.IeeeJointValAttentionFullTrainer(dfs_train, args[\"device\"])\n",
        "    else:\n",
        "      dresults.append(out)\n",
        "      continue\n",
        "    # try:\n",
        "    fchoice[\"ts_sub\"] = ts_sub\n",
        "    out = full_trainer.train(**fchoice)\n",
        "    print(out[\"args\"], out[\"metric\"])\n",
        "    dresults.append(out)\n",
        "    with open(save_path, \"wb\") as f:\n",
        "      pickle.dump(out, f)\n",
        "    # except RuntimeError as e:\n",
        "    #   if isinstance(e, KeyboardInterrupt):\n",
        "    #     raise e\n",
        "    #   else:\n",
        "    #     print(\"####\")\n",
        "    #     print(f\"Failed: {choice}\")\n",
        "    #     print(\"###\")\n",
        "  print(f\"subject: {ts_sub}\")\n",
        "  print(f\"TS:{compute_ensemble(dresults)}\")\n",
        "  aresults.append(dresults)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "subject: 0\n",
            "TS:3.5837992436594845\n",
            "subject: 1\n",
            "TS:3.5164453237299105\n",
            "subject: 2\n",
            "TS:2.901921563960521\n",
            "subject: 3\n",
            "TS:4.78613605398583\n",
            "subject: 4\n",
            "TS:1.8175862372289742\n",
            "subject: 5\n",
            "TS:2.617658858014949\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 4.259 loss_val 4.179 loss_ts 3.669\n",
            "best val epoch: 2\n",
            "[2/30]: loss_train: 3.473 loss_val 3.409 loss_ts 2.747\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 3.436 loss_val 3.394 loss_ts 2.421\n",
            "best val epoch: 6\n",
            "[6/30]: loss_train: 3.454 loss_val 3.393 loss_ts 2.462\n",
            "best val epoch: 7\n",
            "[7/30]: loss_train: 3.390 loss_val 3.338 loss_ts 2.336\n",
            "best val epoch: 8\n",
            "[8/30]: loss_train: 3.376 loss_val 3.327 loss_ts 2.305\n",
            "best val epoch: 11\n",
            "[11/30]: loss_train: 3.350 loss_val 3.293 loss_ts 2.305\n",
            "best val epoch: 12\n",
            "[12/30]: loss_train: 3.312 loss_val 3.260 loss_ts 2.241\n",
            "best val epoch: 13\n",
            "[13/30]: loss_train: 3.282 loss_val 3.232 loss_ts 2.204\n",
            "best val epoch: 15\n",
            "[15/30]: loss_train: 3.239 loss_val 3.179 loss_ts 2.389\n",
            "best val epoch: 17\n",
            "[17/30]: loss_train: 3.109 loss_val 3.050 loss_ts 2.280\n",
            "best val epoch: 19\n",
            "[19/30]: loss_train: 3.096 loss_val 2.971 loss_ts 2.529\n",
            "best val epoch: 22\n",
            "[22/30]: loss_train: 2.962 loss_val 2.841 loss_ts 3.034\n",
            "best val epoch: 26\n",
            "[26/30]: loss_train: 2.680 loss_val 2.626 loss_ts 2.410\n",
            "Final: 2.4100100118059085\n",
            "{'val_sub': 4, 'ts_sub': 6, 'batch_size': 64, 'weight_decay': 0, 'lr': 0.0001, 'lin_dropout': 0, 'lin_size': 16, 'nlin_layers': 2, 'feedforward_expansion': 1, 'nhead': 4, 'ndec_layers': 2, 'nenc_layers': 2, 'conv_dropout': 0, 'nconv_layers': 2, 'conv_filters': 128, 'nfeatures': 5} 2.4100100118059085\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 3.445 loss_val 3.777 loss_ts 2.877\n",
            "best val epoch: 3\n",
            "[3/30]: loss_train: 3.416 loss_val 3.758 loss_ts 2.875\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 3.400 loss_val 3.740 loss_ts 2.752\n",
            "best val epoch: 5\n",
            "[5/30]: loss_train: 3.410 loss_val 3.713 loss_ts 2.791\n",
            "best val epoch: 7\n",
            "[7/30]: loss_train: 3.361 loss_val 3.661 loss_ts 2.542\n",
            "best val epoch: 9\n",
            "[9/30]: loss_train: 3.343 loss_val 3.611 loss_ts 2.394\n",
            "best val epoch: 10\n",
            "[10/30]: loss_train: 3.302 loss_val 3.529 loss_ts 2.520\n",
            "best val epoch: 12\n",
            "[12/30]: loss_train: 3.253 loss_val 3.409 loss_ts 2.373\n",
            "best val epoch: 13\n",
            "[13/30]: loss_train: 3.101 loss_val 3.228 loss_ts 2.375\n",
            "best val epoch: 14\n",
            "[14/30]: loss_train: 3.059 loss_val 3.225 loss_ts 2.715\n",
            "best val epoch: 15\n",
            "[15/30]: loss_train: 3.025 loss_val 3.014 loss_ts 2.583\n",
            "best val epoch: 19\n",
            "[19/30]: loss_train: 2.722 loss_val 2.760 loss_ts 2.460\n",
            "best val epoch: 20\n",
            "[20/30]: loss_train: 2.661 loss_val 2.744 loss_ts 2.712\n",
            "best val epoch: 23\n",
            "[23/30]: loss_train: 2.632 loss_val 2.694 loss_ts 2.457\n",
            "best val epoch: 28\n",
            "[28/30]: loss_train: 2.513 loss_val 2.679 loss_ts 2.681\n",
            "best val epoch: 29\n",
            "[29/30]: loss_train: 2.429 loss_val 2.549 loss_ts 2.693\n",
            "Final: 2.6933089309994043\n",
            "{'val_sub': 4, 'ts_sub': 6, 'batch_size': 64, 'weight_decay': 0, 'lr': 0.0001, 'lin_dropout': 0, 'lin_size': 16, 'nlin_layers': 2, 'feedforward_expansion': 1, 'nhead': 4, 'ndec_layers': 2, 'nenc_layers': 2, 'conv_dropout': 0, 'nconv_layers': 2, 'conv_filters': 128, 'nfeatures': 5} 2.6933089309994043\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 3.496 loss_val 3.735 loss_ts 3.033\n",
            "best val epoch: 3\n",
            "[3/30]: loss_train: 3.432 loss_val 3.639 loss_ts 2.840\n",
            "best val epoch: 4\n",
            "[4/30]: loss_train: 3.313 loss_val 3.460 loss_ts 2.611\n",
            "best val epoch: 5\n",
            "[5/30]: loss_train: 3.245 loss_val 3.407 loss_ts 2.572\n",
            "best val epoch: 6\n",
            "[6/30]: loss_train: 3.167 loss_val 3.303 loss_ts 2.442\n",
            "best val epoch: 7\n",
            "[7/30]: loss_train: 3.102 loss_val 3.251 loss_ts 2.621\n",
            "best val epoch: 11\n",
            "[11/30]: loss_train: 2.969 loss_val 3.135 loss_ts 2.271\n",
            "best val epoch: 12\n",
            "[12/30]: loss_train: 2.610 loss_val 2.824 loss_ts 2.218\n",
            "best val epoch: 13\n",
            "[13/30]: loss_train: 2.644 loss_val 2.745 loss_ts 2.112\n",
            "best val epoch: 15\n",
            "[15/30]: loss_train: 2.567 loss_val 2.714 loss_ts 2.561\n",
            "best val epoch: 18\n",
            "[18/30]: loss_train: 2.318 loss_val 2.478 loss_ts 1.941\n",
            "best val epoch: 19\n",
            "[19/30]: loss_train: 2.271 loss_val 2.454 loss_ts 2.428\n",
            "best val epoch: 20\n",
            "[20/30]: loss_train: 2.228 loss_val 2.328 loss_ts 1.953\n",
            "best val epoch: 23\n",
            "[23/30]: loss_train: 2.113 loss_val 2.271 loss_ts 2.090\n",
            "best val epoch: 26\n",
            "[26/30]: loss_train: 2.048 loss_val 2.254 loss_ts 2.145\n",
            "Final: 2.145017504466126\n",
            "{'val_sub': 4, 'ts_sub': 6, 'batch_size': 64, 'weight_decay': 0, 'lr': 0.0001, 'lin_dropout': 0, 'lin_size': 16, 'nlin_layers': 2, 'feedforward_expansion': 1, 'nhead': 4, 'ndec_layers': 2, 'nenc_layers': 2, 'conv_dropout': 0, 'nconv_layers': 2, 'conv_filters': 128, 'nfeatures': 5} 2.145017504466126\n",
            "best val epoch: 1\n",
            "[1/30]: loss_train: 3.773 loss_val 3.957 loss_ts 3.218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0tSXLWPuCQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}