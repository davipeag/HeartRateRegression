{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "ssh_config = \"\"\"\n",
    "Host github.com\n",
    "  IdentityFile ~/.ssh/github.pem\n",
    "  User davipeag\n",
    "  StrictHostKeyChecking no\n",
    "\"\"\"\n",
    "\n",
    "if os.name == 'nt':\n",
    "  base_path = \"\"\n",
    "  REPO_DIR = \".\"\n",
    "  STORE_DIR =\".\" \n",
    "  print(\"Windows\")\n",
    "else:\n",
    "  print(\"Unix-like\")\n",
    "  REPO_DIR = \"/tmp/HeartRateRegression\"\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  GIT_PATH = \"/content/drive/My\\ Drive/deeplearning_project/github.pem\"\n",
    "  DATA_DIR = os.path.join(REPO_DIR, \"repo\")\n",
    "  STORE_DIR =\"/content/drive/My Drive/deeplearning_project/\" \n",
    "  !mkdir ~/.ssh\n",
    "  !cp -u {GIT_PATH} ~/.ssh/\n",
    "  !chmod u=rw,g=,o= ~/.ssh/github.pem\n",
    "  !echo \"{ssh_config}\" > ~/.ssh/config\n",
    "  !chmod u=rw,g=,o= ~/.ssh/config\n",
    "  ! (cd /tmp && git clone git@github.com:davipeag/HeartRateRegression.git)\n",
    "  ! (cd {REPO_DIR} && git pull )\n",
    "  import sys\n",
    "  sys.path.append(REPO_DIR)\n",
    "\n",
    "def git_pull():\n",
    "  ! (cd {REPO_DIR} && git pull )\n",
    "\n",
    "git_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "args = {\n",
    "    'epoch_num': 250,     # Number of epochs.\n",
    "    'lr': 1.0e-3,           # Learning rate.\n",
    "    'weight_decay': 10e-4, # L2 penalty.\n",
    "    'momentum': 0.9,      # Momentum.\n",
    "    'num_workers': 0,     # Number of workers on data loader.\n",
    "    'batch_size': 128,     # Mini-batch size. 128\n",
    "    'batch_test': 248,     # size of test batch\n",
    "    'window': 15,\n",
    "    'initial_window':5,\n",
    "    'clip_norm': 6.0,     # Upper limit on gradient L2 norm ###\n",
    "}\n",
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "print(args['device'])\n",
    "\n",
    "SEED = 1234\n",
    "def reset_seeds():\n",
    "  random.seed(SEED)\n",
    "  np.random.seed(SEED)\n",
    "  torch.manual_seed(SEED)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.cuda.manual_seed(SEED)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "reset_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import (PpgDaliaExtractor, FormatPPGDalia)\n",
    "\n",
    "extractor = PpgDaliaExtractor(DATA_DIR)\n",
    "ppg_dalia_formatter = FormatPPGDalia()\n",
    "dfs_train = [ppg_dalia_formatter.transform(extractor.extract_subject(i)) for i in range(1,16)]\n",
    "[len(df)//32 for df in dfs_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_pull()\n",
    "\n",
    "import importlib\n",
    "\n",
    "import PPG\n",
    "import preprocessing_utils\n",
    "from PPG import FullTrainer\n",
    "import RegressionHR\n",
    "\n",
    "from RegressionHR import FullTrainer\n",
    "from RegressionHR import PceLstmDefaults\n",
    "from RegressionHR import PceLstmModel\n",
    "from RegressionHR import TrainerJoint\n",
    "from RegressionHR import  UtilitiesData\n",
    "\n",
    "\n",
    "importlib.reload(PPG.AttentionDefaults)\n",
    "importlib.reload(PPG)\n",
    "importlib.reload(PPG.UtilitiesDataXY)\n",
    "importlib.reload(PPG.Models)\n",
    "importlib.reload(PPG.NoHrPceLstmModel)\n",
    "importlib.reload(PPG.TrainerXY)\n",
    "importlib.reload(PPG.TrainerIS)\n",
    "importlib.reload(PPG.FullTrainer)\n",
    "importlib.reload(PceLstmDefaults)\n",
    "importlib.reload(preprocessing_utils)\n",
    "importlib.reload(RegressionHR)\n",
    "importlib.reload(RegressionHR.FullTrainer)\n",
    "importlib.reload(RegressionHR.PceLstmDefaults)\n",
    "importlib.reload(PPG.UtilitiesDataXY)\n",
    "importlib.reload(preprocessing_utils)\n",
    "importlib.reload(RegressionHR.TrainerJoint)\n",
    "importlib.reload(RegressionHR.UtilitiesData)\n",
    "importlib.reload(RegressionHR.PceLstmModel)\n",
    "importlib.reload(preprocessing_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def compute_ensemble(results):\n",
    "  \n",
    "  ys = [v[\"predictions\"][0].reshape(-1).numpy() for v in results]\n",
    "  min_len_y = min([len(y) for y in ys])\n",
    "  ys = [y[:min_len_y] for y in ys]\n",
    "  for i in range(1, len(ys)-1):\n",
    "    assert np.all(ys[i] == ys[i-1])\n",
    "  ps = np.stack([v[\"predictions\"][1].reshape(-1).numpy()[:min_len_y] for v in results])\n",
    "\n",
    "  s = ps[0]\n",
    "  for p in ps[1:]:\n",
    "    s = s + p\n",
    "\n",
    "  a = s/len(ps)\n",
    "  y = ys[0]\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "  plt.plot(a)\n",
    "  plt.plot(y)\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  return np.mean(np.abs(a - y)), np.mean(np.abs(ps - y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fchoice = {\n",
    "    # 'is_h_size': 64,\n",
    "    'ts_per_is': 12,\n",
    "    'period_s': 4,\n",
    "    'step_s': 2,\n",
    "    'ts_per_sample': 50,\n",
    "    'ts_sub': 5,\n",
    "    'batch_size': 64,\n",
    "    'weight_decay': 1e-05,\n",
    "    'lr': 0.005*0.9,\n",
    "    #'nattrs': 40,\n",
    "    'dropout_rate':0.15,\n",
    "    'lstm_input': 128,\n",
    "    'lstm_size': 64,\n",
    "    'ts_h_size': 16,\n",
    "    #'disc_nlayers': 5,\n",
    "    #'disc_layer_size': 64,\n",
    "    #'disc_dropout_rate': 0.15\n",
    "}\n",
    "\n",
    "\n",
    "from PPG import UtilitiesDataXY\n",
    "from collections import defaultdict\n",
    "nepoch = 80\n",
    "aresults = defaultdict(dict)\n",
    "for val_sub in [4,5,3,2,1,0,6,7]:\n",
    "  for ts_sub in range(15):\n",
    "    if val_sub == ts_sub:\n",
    "      continue\n",
    "    fchoice[\"ts_sub\"] = ts_sub\n",
    "    # fchoice[\"val_sub\"] = val_sub\n",
    "    filename = f\"dalia_ts_no_pce_{ts_sub}_val_{val_sub}_nepoch_{nepoch}.pkl\"\n",
    "    save_path = os.path.join(STORE_DIR, filename)\n",
    "    try:\n",
    "      with open(save_path , \"rb\") as f:\n",
    "        out = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "      full_trainer = RegressionHR.FullTrainer.NoPceLstmDaliaFullTrainerJointValidation(dfs_train, args[\"device\"], nepoch)\n",
    "      try:\n",
    "        out = full_trainer.train(**fchoice)\n",
    "        with open(save_path, \"wb\") as f:\n",
    "          pickle.dump(out, f)\n",
    "\n",
    "      except RuntimeError as e:\n",
    "        if isinstance(e, KeyboardInterrupt):\n",
    "          raise e\n",
    "        else:\n",
    "          print(\"####\")\n",
    "          print(f\"Failed: {choice}\")\n",
    "          print(\"###\")\n",
    "    \n",
    "    print(out[\"args\"], out[\"metric\"])\n",
    "    aresults[ts_sub][val_sub] = out\n",
    "    print(f\"{ts_sub}-TS:{compute_ensemble(list(aresults[ts_sub].values()))}\")\n",
    "    \n",
    "\n"
   ]
  }
 ]
}